{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dd202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.0, 1550, 4.464], [-10.0, 1850, 4.464], [10.0, 1550, 4.464], [10.0, 1850, 4.464], [30.0, 1550, 4.464], [30.0, 1850, 4.464], [-10.0, 1550, 6.696], [-10.0, 1850, 6.696], [10.0, 1550, 6.696], [10.0, 1850, 6.696], [30.0, 1550, 6.696], [30.0, 1850, 6.696], [-10.0, 1550, 8.928], [-10.0, 1850, 8.928], [10.0, 1550, 8.928], [10.0, 1850, 8.928], [30.0, 1550, 8.928], [30.0, 1850, 8.928]]\n",
      "[[ -10.    1550.       4.464]\n",
      " [ -10.    1850.       4.464]\n",
      " [  10.    1550.       4.464]\n",
      " [  10.    1850.       4.464]\n",
      " [  30.    1550.       4.464]\n",
      " [  30.    1850.       4.464]\n",
      " [ -10.    1550.       6.696]\n",
      " [ -10.    1850.       6.696]\n",
      " [  10.    1550.       6.696]\n",
      " [  10.    1850.       6.696]\n",
      " [  30.    1550.       6.696]\n",
      " [  30.    1850.       6.696]\n",
      " [ -10.    1550.       8.928]\n",
      " [ -10.    1850.       8.928]\n",
      " [  10.    1550.       8.928]\n",
      " [  10.    1850.       8.928]\n",
      " [  30.    1550.       8.928]\n",
      " [  30.    1850.       8.928]]\n",
      "[[26.1, 152.8], [26.5, 172.7], [27.9, 191.4], [28.3, 198.0], [29.6, 217.8], [30.1, 224.4], [26.9, 118.8], [27.2, 122.1], [28.8, 136.4], [29.1, 139.7], [30.6, 154.0], [31.0, 158.4], [27.3, 92.4], [27.6, 94.6], [29.2, 105.6], [29.6, 107.8], [31.1, 119.9], [31.5, 123.2]]\n",
      "[[ 26.1 152.8]\n",
      " [ 26.5 172.7]\n",
      " [ 27.9 191.4]\n",
      " [ 28.3 198. ]\n",
      " [ 29.6 217.8]\n",
      " [ 30.1 224.4]\n",
      " [ 26.9 118.8]\n",
      " [ 27.2 122.1]\n",
      " [ 28.8 136.4]\n",
      " [ 29.1 139.7]\n",
      " [ 30.6 154. ]\n",
      " [ 31.  158.4]\n",
      " [ 27.3  92.4]\n",
      " [ 27.6  94.6]\n",
      " [ 29.2 105.6]\n",
      " [ 29.6 107.8]\n",
      " [ 31.1 119.9]\n",
      " [ 31.5 123.2]]\n"
     ]
    }
   ],
   "source": [
    "import math, numpy\n",
    "#Part 1 input data: Air temp (degC), ID (W/sqm), load resistance (ohms)\n",
    "# - split into training set and a randomly slected small validation set\n",
    "xdata = [[-10.0, 1550, 4.464], \n",
    "  [-10.0, 1850, 4.464], \n",
    "  [10.0, 1550, 4.464], \n",
    "  [10.0, 1850, 4.464], \n",
    "  [30.0, 1550, 4.464], \n",
    "  [30.0, 1850, 4.464], \n",
    "  [-10.0, 1550, 6.696], \n",
    "  [-10.0, 1850, 6.696], \n",
    "  [10.0, 1550, 6.696], \n",
    "  [10.0, 1850, 6.696], \n",
    "  [30.0, 1550, 6.696], \n",
    "  [30.0, 1850, 6.696], \n",
    "  [-10.0, 1550, 8.928], \n",
    "  [-10.0, 1850, 8.928],   \n",
    "  [10.0, 1550, 8.928], \n",
    "  [10.0, 1850, 8.928], \n",
    "  [30.0, 1550, 8.928], \n",
    "  [30.0, 1850, 8.928]]\n",
    "\n",
    "\n",
    "#Part 1 output data: VL (V) and Power out (W)\n",
    "ydata = [[26.1, 152.8], \n",
    " [26.5, 172.7], \n",
    " [27.9, 191.4], \n",
    " [28.3, 198.0], \n",
    " [29.6, 217.8], \n",
    " [30.1, 224.4],  \n",
    " [26.9, 118.8], \n",
    " [27.2, 122.1], \n",
    " [28.8, 136.4], \n",
    " [29.1, 139.7], \n",
    " [30.6, 154.0], \n",
    " [31.0, 158.4],  \n",
    " [27.3, 92.4], \n",
    " [27.6, 94.6], \n",
    " [29.2, 105.6], \n",
    " [29.6, 107.8], \n",
    " [31.1, 119.9], \n",
    " [31.5, 123.2]]\n",
    "\n",
    "\n",
    "xarray= numpy.array(xdata)\n",
    "yarray= numpy.array(ydata)\n",
    "print (xdata)\n",
    "print (xarray)\n",
    "print (ydata)\n",
    "print (yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314420ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a7ce36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 1700.0, 6.696]\n",
      "[28.950000000000003, 138.05]\n"
     ]
    }
   ],
   "source": [
    "Tamed=np.median(xarray[:,0])\n",
    "IDmed=np.median(xarray[:,1])\n",
    "RLmed=np.median(xarray[:,2])\n",
    "VLmed=np.median(yarray[:,0])\n",
    "Wdmed=np.median(yarray[:,1])\n",
    "\n",
    "Xmedian = [Tamed,IDmed,RLmed]\n",
    "Ymedian = [VLmed,Wdmed]\n",
    "print(Xmedian)\n",
    "print(Ymedian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd39a2",
   "metadata": {},
   "source": [
    "## normalize the data by dividing each parameter value by its median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b585b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          0.91176471  0.66666667]\n",
      " [-1.          1.08823529  0.66666667]\n",
      " [ 1.          0.91176471  0.66666667]\n",
      " [ 1.          1.08823529  0.66666667]\n",
      " [ 3.          0.91176471  0.66666667]\n",
      " [ 3.          1.08823529  0.66666667]\n",
      " [-1.          0.91176471  1.        ]\n",
      " [-1.          1.08823529  1.        ]\n",
      " [ 1.          0.91176471  1.        ]\n",
      " [ 1.          1.08823529  1.        ]\n",
      " [ 3.          0.91176471  1.        ]\n",
      " [ 3.          1.08823529  1.        ]\n",
      " [-1.          0.91176471  1.33333333]\n",
      " [-1.          1.08823529  1.33333333]\n",
      " [ 1.          0.91176471  1.33333333]\n",
      " [ 1.          1.08823529  1.33333333]\n",
      " [ 3.          0.91176471  1.33333333]\n",
      " [ 3.          1.08823529  1.33333333]]\n",
      "[[0.9015544  1.10684535]\n",
      " [0.91537133 1.25099602]\n",
      " [0.96373057 1.38645418]\n",
      " [0.9775475  1.43426295]\n",
      " [1.0224525  1.57768924]\n",
      " [1.03972366 1.62549801]\n",
      " [0.92918826 0.86055777]\n",
      " [0.93955095 0.88446215]\n",
      " [0.99481865 0.98804781]\n",
      " [1.00518135 1.01195219]\n",
      " [1.05699482 1.11553785]\n",
      " [1.07081174 1.14741036]\n",
      " [0.94300518 0.66932271]\n",
      " [0.95336788 0.68525896]\n",
      " [1.00863558 0.76494024]\n",
      " [1.0224525  0.78087649]\n",
      " [1.07426598 0.8685259 ]\n",
      " [1.0880829  0.89243028]]\n"
     ]
    }
   ],
   "source": [
    "Xtask2=xarray/Xmedian\n",
    "Ytask2=yarray/Ymedian\n",
    "print(Xtask2)\n",
    "print(Ytask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f170375",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8459425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xtask2, Ytask2, test_size=1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88906dc",
   "metadata": {},
   "source": [
    "##  (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1817e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2021-11-11 17:50:21.382461: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-11 17:50:21.382787: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "#2.4\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(6, activation=K.elu, input_shape=[3],  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu,  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(16, activation=K.elu,  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu,  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(2,  kernel_initializer=initializer)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2634ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.0150)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ddeb2",
   "metadata": {},
   "source": [
    "## (d) smallest lose要小于0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128480d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.6468\n",
      "Epoch 2/400\n",
      "12/12 [==============================] - 0s 556us/step - loss: 0.6164\n",
      "Epoch 3/400\n",
      "12/12 [==============================] - 0s 598us/step - loss: 0.8167\n",
      "Epoch 4/400\n",
      "12/12 [==============================] - 0s 666us/step - loss: 0.6506\n",
      "Epoch 5/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3932\n",
      "Epoch 6/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2055\n",
      "Epoch 7/400\n",
      "12/12 [==============================] - 0s 746us/step - loss: 0.2089\n",
      "Epoch 8/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3011\n",
      "Epoch 9/400\n",
      "12/12 [==============================] - 0s 723us/step - loss: 0.1932\n",
      "Epoch 10/400\n",
      "12/12 [==============================] - 0s 839us/step - loss: 0.2586\n",
      "Epoch 11/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2405\n",
      "Epoch 12/400\n",
      "12/12 [==============================] - 0s 763us/step - loss: 0.2953\n",
      "Epoch 13/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1761\n",
      "Epoch 14/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2446\n",
      "Epoch 15/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2054\n",
      "Epoch 16/400\n",
      "12/12 [==============================] - 0s 906us/step - loss: 0.3937\n",
      "Epoch 17/400\n",
      "12/12 [==============================] - 0s 538us/step - loss: 0.1552\n",
      "Epoch 18/400\n",
      "12/12 [==============================] - 0s 419us/step - loss: 0.1567\n",
      "Epoch 19/400\n",
      "12/12 [==============================] - 0s 779us/step - loss: 0.3036\n",
      "Epoch 20/400\n",
      "12/12 [==============================] - 0s 346us/step - loss: 0.1697\n",
      "Epoch 21/400\n",
      "12/12 [==============================] - 0s 372us/step - loss: 0.2657\n",
      "Epoch 22/400\n",
      "12/12 [==============================] - 0s 368us/step - loss: 0.3334\n",
      "Epoch 23/400\n",
      "12/12 [==============================] - 0s 344us/step - loss: 0.1782\n",
      "Epoch 24/400\n",
      "12/12 [==============================] - 0s 294us/step - loss: 0.2931\n",
      "Epoch 25/400\n",
      "12/12 [==============================] - 0s 390us/step - loss: 0.1405\n",
      "Epoch 26/400\n",
      "12/12 [==============================] - 0s 392us/step - loss: 0.2747\n",
      "Epoch 27/400\n",
      "12/12 [==============================] - 0s 346us/step - loss: 0.2343\n",
      "Epoch 28/400\n",
      "12/12 [==============================] - 0s 265us/step - loss: 0.1850\n",
      "Epoch 29/400\n",
      "12/12 [==============================] - 0s 293us/step - loss: 0.1986\n",
      "Epoch 30/400\n",
      "12/12 [==============================] - 0s 280us/step - loss: 0.1489\n",
      "Epoch 31/400\n",
      "12/12 [==============================] - 0s 314us/step - loss: 0.2022\n",
      "Epoch 32/400\n",
      "12/12 [==============================] - 0s 265us/step - loss: 0.1983\n",
      "Epoch 33/400\n",
      "12/12 [==============================] - 0s 315us/step - loss: 0.3809\n",
      "Epoch 34/400\n",
      "12/12 [==============================] - 0s 255us/step - loss: 0.1620\n",
      "Epoch 35/400\n",
      "12/12 [==============================] - 0s 326us/step - loss: 0.1870\n",
      "Epoch 36/400\n",
      "12/12 [==============================] - 0s 244us/step - loss: 0.1723\n",
      "Epoch 37/400\n",
      "12/12 [==============================] - 0s 233us/step - loss: 0.1623\n",
      "Epoch 38/400\n",
      "12/12 [==============================] - 0s 216us/step - loss: 0.2241\n",
      "Epoch 39/400\n",
      "12/12 [==============================] - 0s 220us/step - loss: 0.1080\n",
      "Epoch 40/400\n",
      "12/12 [==============================] - 0s 421us/step - loss: 0.1958\n",
      "Epoch 41/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0982\n",
      "Epoch 42/400\n",
      "12/12 [==============================] - 0s 490us/step - loss: 0.1726\n",
      "Epoch 43/400\n",
      "12/12 [==============================] - 0s 273us/step - loss: 0.3599\n",
      "Epoch 44/400\n",
      "12/12 [==============================] - 0s 310us/step - loss: 0.2137\n",
      "Epoch 45/400\n",
      "12/12 [==============================] - 0s 230us/step - loss: 0.1360\n",
      "Epoch 46/400\n",
      "12/12 [==============================] - 0s 295us/step - loss: 0.1820\n",
      "Epoch 47/400\n",
      "12/12 [==============================] - 0s 287us/step - loss: 0.1007\n",
      "Epoch 48/400\n",
      "12/12 [==============================] - 0s 330us/step - loss: 0.1274\n",
      "Epoch 49/400\n",
      "12/12 [==============================] - 0s 409us/step - loss: 0.1395\n",
      "Epoch 50/400\n",
      "12/12 [==============================] - 0s 356us/step - loss: 0.1048\n",
      "Epoch 51/400\n",
      "12/12 [==============================] - 0s 795us/step - loss: 0.1265\n",
      "Epoch 52/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1289\n",
      "Epoch 53/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2633\n",
      "Epoch 54/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1429\n",
      "Epoch 55/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1606\n",
      "Epoch 56/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0893\n",
      "Epoch 57/400\n",
      "12/12 [==============================] - 0s 850us/step - loss: 0.1428\n",
      "Epoch 58/400\n",
      "12/12 [==============================] - 0s 446us/step - loss: 0.2119\n",
      "Epoch 59/400\n",
      "12/12 [==============================] - 0s 401us/step - loss: 0.3097\n",
      "Epoch 60/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1167\n",
      "Epoch 61/400\n",
      "12/12 [==============================] - 0s 311us/step - loss: 0.1419\n",
      "Epoch 62/400\n",
      "12/12 [==============================] - 0s 731us/step - loss: 0.0991\n",
      "Epoch 63/400\n",
      "12/12 [==============================] - 0s 834us/step - loss: 0.1645\n",
      "Epoch 64/400\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.3054\n",
      "Epoch 65/400\n",
      "12/12 [==============================] - 0s 563us/step - loss: 0.0990\n",
      "Epoch 66/400\n",
      "12/12 [==============================] - 0s 296us/step - loss: 0.0848\n",
      "Epoch 67/400\n",
      "12/12 [==============================] - 0s 263us/step - loss: 0.1169\n",
      "Epoch 68/400\n",
      "12/12 [==============================] - 0s 582us/step - loss: 0.0745\n",
      "Epoch 69/400\n",
      "12/12 [==============================] - 0s 326us/step - loss: 0.1427\n",
      "Epoch 70/400\n",
      "12/12 [==============================] - 0s 541us/step - loss: 0.0951\n",
      "Epoch 71/400\n",
      "12/12 [==============================] - 0s 292us/step - loss: 0.1567\n",
      "Epoch 72/400\n",
      "12/12 [==============================] - 0s 268us/step - loss: 0.2520\n",
      "Epoch 73/400\n",
      "12/12 [==============================] - 0s 297us/step - loss: 0.1061\n",
      "Epoch 74/400\n",
      "12/12 [==============================] - 0s 530us/step - loss: 0.1256\n",
      "Epoch 75/400\n",
      "12/12 [==============================] - 0s 328us/step - loss: 0.1226\n",
      "Epoch 76/400\n",
      "12/12 [==============================] - 0s 451us/step - loss: 0.1043\n",
      "Epoch 77/400\n",
      "12/12 [==============================] - 0s 442us/step - loss: 0.1003\n",
      "Epoch 78/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2116\n",
      "Epoch 79/400\n",
      "12/12 [==============================] - 0s 472us/step - loss: 0.1195\n",
      "Epoch 80/400\n",
      "12/12 [==============================] - 0s 706us/step - loss: 0.2126\n",
      "Epoch 81/400\n",
      "12/12 [==============================] - 0s 401us/step - loss: 0.0940\n",
      "Epoch 82/400\n",
      "12/12 [==============================] - 0s 312us/step - loss: 0.0674\n",
      "Epoch 83/400\n",
      "12/12 [==============================] - 0s 226us/step - loss: 0.0894\n",
      "Epoch 84/400\n",
      "12/12 [==============================] - 0s 330us/step - loss: 0.0937\n",
      "Epoch 85/400\n",
      "12/12 [==============================] - 0s 413us/step - loss: 0.2179\n",
      "Epoch 86/400\n",
      "12/12 [==============================] - 0s 351us/step - loss: 0.0993\n",
      "Epoch 87/400\n",
      "12/12 [==============================] - 0s 359us/step - loss: 0.1273\n",
      "Epoch 88/400\n",
      "12/12 [==============================] - 0s 494us/step - loss: 0.0726\n",
      "Epoch 89/400\n",
      "12/12 [==============================] - 0s 486us/step - loss: 0.2256\n",
      "Epoch 90/400\n",
      "12/12 [==============================] - 0s 289us/step - loss: 0.0863\n",
      "Epoch 91/400\n",
      "12/12 [==============================] - 0s 443us/step - loss: 0.0553\n",
      "Epoch 92/400\n",
      "12/12 [==============================] - 0s 246us/step - loss: 0.1085\n",
      "Epoch 93/400\n",
      "12/12 [==============================] - 0s 621us/step - loss: 0.0889\n",
      "Epoch 94/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0671\n",
      "Epoch 95/400\n",
      "12/12 [==============================] - 0s 519us/step - loss: 0.1198\n",
      "Epoch 96/400\n",
      "12/12 [==============================] - 0s 350us/step - loss: 0.2241\n",
      "Epoch 97/400\n",
      "12/12 [==============================] - 0s 432us/step - loss: 0.1117\n",
      "Epoch 98/400\n",
      "12/12 [==============================] - 0s 329us/step - loss: 0.1658\n",
      "Epoch 99/400\n",
      "12/12 [==============================] - 0s 560us/step - loss: 0.1435\n",
      "Epoch 100/400\n",
      "12/12 [==============================] - 0s 311us/step - loss: 0.0956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/400\n",
      "12/12 [==============================] - 0s 330us/step - loss: 0.0569\n",
      "Epoch 102/400\n",
      "12/12 [==============================] - 0s 269us/step - loss: 0.1072\n",
      "Epoch 103/400\n",
      "12/12 [==============================] - 0s 312us/step - loss: 0.1702\n",
      "Epoch 104/400\n",
      "12/12 [==============================] - 0s 330us/step - loss: 0.1209\n",
      "Epoch 105/400\n",
      "12/12 [==============================] - 0s 294us/step - loss: 0.0916\n",
      "Epoch 106/400\n",
      "12/12 [==============================] - 0s 401us/step - loss: 0.0648\n",
      "Epoch 107/400\n",
      "12/12 [==============================] - 0s 315us/step - loss: 0.0883\n",
      "Epoch 108/400\n",
      "12/12 [==============================] - 0s 332us/step - loss: 0.0866\n",
      "Epoch 109/400\n",
      "12/12 [==============================] - 0s 344us/step - loss: 0.2335\n",
      "Epoch 110/400\n",
      "12/12 [==============================] - 0s 337us/step - loss: 0.0809\n",
      "Epoch 111/400\n",
      "12/12 [==============================] - 0s 310us/step - loss: 0.0362\n",
      "Epoch 112/400\n",
      "12/12 [==============================] - 0s 322us/step - loss: 0.1425\n",
      "Epoch 113/400\n",
      "12/12 [==============================] - 0s 861us/step - loss: 0.1463\n",
      "Epoch 114/400\n",
      "12/12 [==============================] - 0s 454us/step - loss: 0.1353\n",
      "Epoch 115/400\n",
      "12/12 [==============================] - 0s 415us/step - loss: 0.0902\n",
      "Epoch 116/400\n",
      "12/12 [==============================] - 0s 264us/step - loss: 0.0607\n",
      "Epoch 117/400\n",
      "12/12 [==============================] - 0s 272us/step - loss: 0.1810\n",
      "Epoch 118/400\n",
      "12/12 [==============================] - 0s 276us/step - loss: 0.1445\n",
      "Epoch 119/400\n",
      "12/12 [==============================] - 0s 356us/step - loss: 0.0628\n",
      "Epoch 120/400\n",
      "12/12 [==============================] - 0s 284us/step - loss: 0.0855\n",
      "Epoch 121/400\n",
      "12/12 [==============================] - 0s 282us/step - loss: 0.2337\n",
      "Epoch 122/400\n",
      "12/12 [==============================] - 0s 273us/step - loss: 0.0746\n",
      "Epoch 123/400\n",
      "12/12 [==============================] - 0s 824us/step - loss: 0.0806\n",
      "Epoch 124/400\n",
      "12/12 [==============================] - 0s 335us/step - loss: 0.1089\n",
      "Epoch 125/400\n",
      "12/12 [==============================] - 0s 948us/step - loss: 0.2096\n",
      "Epoch 126/400\n",
      "12/12 [==============================] - 0s 887us/step - loss: 0.0703\n",
      "Epoch 127/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 128/400\n",
      "12/12 [==============================] - 0s 994us/step - loss: 0.1407\n",
      "Epoch 129/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1274\n",
      "Epoch 130/400\n",
      "12/12 [==============================] - 0s 417us/step - loss: 0.0664\n",
      "Epoch 131/400\n",
      "12/12 [==============================] - 0s 711us/step - loss: 0.0606\n",
      "Epoch 132/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1430\n",
      "Epoch 133/400\n",
      "12/12 [==============================] - 0s 849us/step - loss: 0.0515\n",
      "Epoch 134/400\n",
      "12/12 [==============================] - 0s 279us/step - loss: 0.0419\n",
      "Epoch 135/400\n",
      "12/12 [==============================] - 0s 986us/step - loss: 0.0962\n",
      "Epoch 136/400\n",
      "12/12 [==============================] - 0s 587us/step - loss: 0.0855\n",
      "Epoch 137/400\n",
      "12/12 [==============================] - 0s 450us/step - loss: 0.0798\n",
      "Epoch 138/400\n",
      "12/12 [==============================] - 0s 346us/step - loss: 0.1221\n",
      "Epoch 139/400\n",
      "12/12 [==============================] - 0s 391us/step - loss: 0.1808\n",
      "Epoch 140/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0826\n",
      "Epoch 141/400\n",
      "12/12 [==============================] - 0s 328us/step - loss: 0.0493\n",
      "Epoch 142/400\n",
      "12/12 [==============================] - 0s 675us/step - loss: 0.0883\n",
      "Epoch 143/400\n",
      "12/12 [==============================] - 0s 806us/step - loss: 0.1535\n",
      "Epoch 144/400\n",
      "12/12 [==============================] - 0s 475us/step - loss: 0.1254\n",
      "Epoch 145/400\n",
      "12/12 [==============================] - 0s 996us/step - loss: 0.0996\n",
      "Epoch 146/400\n",
      "12/12 [==============================] - 0s 289us/step - loss: 0.0407\n",
      "Epoch 147/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1854\n",
      "Epoch 148/400\n",
      "12/12 [==============================] - 0s 294us/step - loss: 0.0768\n",
      "Epoch 149/400\n",
      "12/12 [==============================] - 0s 612us/step - loss: 0.1634\n",
      "Epoch 150/400\n",
      "12/12 [==============================] - 0s 393us/step - loss: 0.0784\n",
      "Epoch 151/400\n",
      "12/12 [==============================] - 0s 550us/step - loss: 0.0503\n",
      "Epoch 152/400\n",
      "12/12 [==============================] - 0s 718us/step - loss: 0.0643\n",
      "Epoch 153/400\n",
      "12/12 [==============================] - 0s 802us/step - loss: 0.1075\n",
      "Epoch 154/400\n",
      "12/12 [==============================] - 0s 327us/step - loss: 0.1903\n",
      "Epoch 155/400\n",
      "12/12 [==============================] - 0s 259us/step - loss: 0.0947\n",
      "Epoch 156/400\n",
      "12/12 [==============================] - 0s 229us/step - loss: 0.0787\n",
      "Epoch 157/400\n",
      "12/12 [==============================] - 0s 601us/step - loss: 0.0537\n",
      "Epoch 158/400\n",
      "12/12 [==============================] - 0s 913us/step - loss: 0.1034\n",
      "Epoch 159/400\n",
      "12/12 [==============================] - 0s 677us/step - loss: 0.0710\n",
      "Epoch 160/400\n",
      "12/12 [==============================] - 0s 520us/step - loss: 0.1700\n",
      "Epoch 161/400\n",
      "12/12 [==============================] - 0s 615us/step - loss: 0.0803\n",
      "Epoch 162/400\n",
      "12/12 [==============================] - 0s 644us/step - loss: 0.0333\n",
      "Epoch 163/400\n",
      "12/12 [==============================] - 0s 303us/step - loss: 0.0906\n",
      "Epoch 164/400\n",
      "12/12 [==============================] - 0s 290us/step - loss: 0.0625\n",
      "Epoch 165/400\n",
      "12/12 [==============================] - 0s 437us/step - loss: 0.1492\n",
      "Epoch 166/400\n",
      "12/12 [==============================] - 0s 395us/step - loss: 0.1363\n",
      "Epoch 167/400\n",
      "12/12 [==============================] - 0s 822us/step - loss: 0.0751\n",
      "Epoch 168/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0813\n",
      "Epoch 169/400\n",
      "12/12 [==============================] - 0s 577us/step - loss: 0.1289\n",
      "Epoch 170/400\n",
      "12/12 [==============================] - 0s 353us/step - loss: 0.0692\n",
      "Epoch 171/400\n",
      "12/12 [==============================] - 0s 375us/step - loss: 0.0875\n",
      "Epoch 172/400\n",
      "12/12 [==============================] - 0s 305us/step - loss: 0.0759\n",
      "Epoch 173/400\n",
      "12/12 [==============================] - 0s 470us/step - loss: 0.0984\n",
      "Epoch 174/400\n",
      "12/12 [==============================] - 0s 421us/step - loss: 0.0570\n",
      "Epoch 175/400\n",
      "12/12 [==============================] - 0s 295us/step - loss: 0.0518\n",
      "Epoch 176/400\n",
      "12/12 [==============================] - 0s 386us/step - loss: 0.1417\n",
      "Epoch 177/400\n",
      "12/12 [==============================] - 0s 267us/step - loss: 0.1102\n",
      "Epoch 178/400\n",
      "12/12 [==============================] - 0s 401us/step - loss: 0.0631\n",
      "Epoch 179/400\n",
      "12/12 [==============================] - 0s 344us/step - loss: 0.0861\n",
      "Epoch 180/400\n",
      "12/12 [==============================] - 0s 261us/step - loss: 0.1274\n",
      "Epoch 181/400\n",
      "12/12 [==============================] - 0s 273us/step - loss: 0.0815\n",
      "Epoch 182/400\n",
      "12/12 [==============================] - 0s 387us/step - loss: 0.0545\n",
      "Epoch 183/400\n",
      "12/12 [==============================] - 0s 345us/step - loss: 0.0663\n",
      "Epoch 184/400\n",
      "12/12 [==============================] - 0s 603us/step - loss: 0.1042\n",
      "Epoch 185/400\n",
      "12/12 [==============================] - 0s 292us/step - loss: 0.1044\n",
      "Epoch 186/400\n",
      "12/12 [==============================] - 0s 311us/step - loss: 0.0712\n",
      "Epoch 187/400\n",
      "12/12 [==============================] - 0s 553us/step - loss: 0.1900\n",
      "Epoch 188/400\n",
      "12/12 [==============================] - 0s 490us/step - loss: 0.1021\n",
      "Epoch 189/400\n",
      "12/12 [==============================] - 0s 293us/step - loss: 0.0530\n",
      "Epoch 190/400\n",
      "12/12 [==============================] - 0s 696us/step - loss: 0.0437\n",
      "Epoch 191/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1776\n",
      "Epoch 192/400\n",
      "12/12 [==============================] - 0s 693us/step - loss: 0.1148\n",
      "Epoch 193/400\n",
      "12/12 [==============================] - 0s 563us/step - loss: 0.0866\n",
      "Epoch 194/400\n",
      "12/12 [==============================] - 0s 410us/step - loss: 0.0620\n",
      "Epoch 195/400\n",
      "12/12 [==============================] - 0s 883us/step - loss: 0.1028\n",
      "Epoch 196/400\n",
      "12/12 [==============================] - 0s 447us/step - loss: 0.0806\n",
      "Epoch 197/400\n",
      "12/12 [==============================] - 0s 280us/step - loss: 0.0903\n",
      "Epoch 198/400\n",
      "12/12 [==============================] - 0s 313us/step - loss: 0.0444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/400\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.0413\n",
      "Epoch 200/400\n",
      "12/12 [==============================] - 0s 414us/step - loss: 0.0970\n",
      "Epoch 201/400\n",
      "12/12 [==============================] - 0s 320us/step - loss: 0.0655\n",
      "Epoch 202/400\n",
      "12/12 [==============================] - 0s 365us/step - loss: 0.2118\n",
      "Epoch 203/400\n",
      "12/12 [==============================] - 0s 343us/step - loss: 0.0961\n",
      "Epoch 204/400\n",
      "12/12 [==============================] - 0s 224us/step - loss: 0.0568\n",
      "Epoch 205/400\n",
      "12/12 [==============================] - 0s 273us/step - loss: 0.0851\n",
      "Epoch 206/400\n",
      "12/12 [==============================] - 0s 261us/step - loss: 0.0823\n",
      "Epoch 207/400\n",
      "12/12 [==============================] - 0s 253us/step - loss: 0.0486\n",
      "Epoch 208/400\n",
      "12/12 [==============================] - 0s 246us/step - loss: 0.0658\n",
      "Epoch 209/400\n",
      "12/12 [==============================] - 0s 382us/step - loss: 0.0646\n",
      "Epoch 210/400\n",
      "12/12 [==============================] - 0s 382us/step - loss: 0.0824\n",
      "Epoch 211/400\n",
      "12/12 [==============================] - 0s 276us/step - loss: 0.1745\n",
      "Epoch 212/400\n",
      "12/12 [==============================] - 0s 517us/step - loss: 0.0651\n",
      "Epoch 213/400\n",
      "12/12 [==============================] - 0s 386us/step - loss: 0.0198\n",
      "Epoch 214/400\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.0729\n",
      "Epoch 215/400\n",
      "12/12 [==============================] - 0s 233us/step - loss: 0.1490\n",
      "Epoch 216/400\n",
      "12/12 [==============================] - 0s 238us/step - loss: 0.1092\n",
      "Epoch 217/400\n",
      "12/12 [==============================] - 0s 262us/step - loss: 0.0861\n",
      "Epoch 218/400\n",
      "12/12 [==============================] - 0s 267us/step - loss: 0.0968\n",
      "Epoch 219/400\n",
      "12/12 [==============================] - 0s 272us/step - loss: 0.0875\n",
      "Epoch 220/400\n",
      "12/12 [==============================] - 0s 232us/step - loss: 0.0677\n",
      "Epoch 221/400\n",
      "12/12 [==============================] - 0s 318us/step - loss: 0.1210\n",
      "Epoch 222/400\n",
      "12/12 [==============================] - 0s 705us/step - loss: 0.0612\n",
      "Epoch 223/400\n",
      "12/12 [==============================] - 0s 631us/step - loss: 0.0613\n",
      "Epoch 224/400\n",
      "12/12 [==============================] - 0s 438us/step - loss: 0.1316\n",
      "Epoch 225/400\n",
      "12/12 [==============================] - 0s 984us/step - loss: 0.0885\n",
      "Epoch 226/400\n",
      "12/12 [==============================] - 0s 466us/step - loss: 0.1213\n",
      "Epoch 227/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0565\n",
      "Epoch 228/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0443\n",
      "Epoch 229/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0766\n",
      "Epoch 230/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 231/400\n",
      "12/12 [==============================] - 0s 846us/step - loss: 0.1162\n",
      "Epoch 232/400\n",
      "12/12 [==============================] - 0s 653us/step - loss: 0.0456\n",
      "Epoch 233/400\n",
      "12/12 [==============================] - 0s 634us/step - loss: 0.0810\n",
      "Epoch 234/400\n",
      "12/12 [==============================] - 0s 489us/step - loss: 0.1152\n",
      "Epoch 235/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1317\n",
      "Epoch 236/400\n",
      "12/12 [==============================] - 0s 688us/step - loss: 0.0671\n",
      "Epoch 237/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0358\n",
      "Epoch 238/400\n",
      "12/12 [==============================] - 0s 537us/step - loss: 0.0723\n",
      "Epoch 239/400\n",
      "12/12 [==============================] - 0s 447us/step - loss: 0.0681\n",
      "Epoch 240/400\n",
      "12/12 [==============================] - 0s 263us/step - loss: 0.1549\n",
      "Epoch 241/400\n",
      "12/12 [==============================] - 0s 661us/step - loss: 0.0684\n",
      "Epoch 242/400\n",
      "12/12 [==============================] - 0s 844us/step - loss: 0.0234\n",
      "Epoch 243/400\n",
      "12/12 [==============================] - 0s 468us/step - loss: 0.0489\n",
      "Epoch 244/400\n",
      "12/12 [==============================] - 0s 592us/step - loss: 0.1530\n",
      "Epoch 245/400\n",
      "12/12 [==============================] - 0s 554us/step - loss: 0.0602\n",
      "Epoch 246/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0701\n",
      "Epoch 247/400\n",
      "12/12 [==============================] - 0s 493us/step - loss: 0.1477\n",
      "Epoch 248/400\n",
      "12/12 [==============================] - 0s 835us/step - loss: 0.0758\n",
      "Epoch 249/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1316\n",
      "Epoch 250/400\n",
      "12/12 [==============================] - 0s 517us/step - loss: 0.0672\n",
      "Epoch 251/400\n",
      "12/12 [==============================] - 0s 528us/step - loss: 0.0300\n",
      "Epoch 252/400\n",
      "12/12 [==============================] - 0s 480us/step - loss: 0.1145\n",
      "Epoch 253/400\n",
      "12/12 [==============================] - 0s 407us/step - loss: 0.0720\n",
      "Epoch 254/400\n",
      "12/12 [==============================] - 0s 335us/step - loss: 0.0999\n",
      "Epoch 255/400\n",
      "12/12 [==============================] - 0s 369us/step - loss: 0.0752\n",
      "Epoch 256/400\n",
      "12/12 [==============================] - 0s 572us/step - loss: 0.0536\n",
      "Epoch 257/400\n",
      "12/12 [==============================] - 0s 439us/step - loss: 0.0872\n",
      "Epoch 258/400\n",
      "12/12 [==============================] - 0s 530us/step - loss: 0.0460\n",
      "Epoch 259/400\n",
      "12/12 [==============================] - 0s 716us/step - loss: 0.0733\n",
      "Epoch 260/400\n",
      "12/12 [==============================] - 0s 418us/step - loss: 0.0654\n",
      "Epoch 261/400\n",
      "12/12 [==============================] - 0s 321us/step - loss: 0.0844\n",
      "Epoch 262/400\n",
      "12/12 [==============================] - 0s 374us/step - loss: 0.1431\n",
      "Epoch 263/400\n",
      "12/12 [==============================] - 0s 286us/step - loss: 0.0611\n",
      "Epoch 264/400\n",
      "12/12 [==============================] - 0s 315us/step - loss: 0.0427\n",
      "Epoch 265/400\n",
      "12/12 [==============================] - 0s 865us/step - loss: 0.1296\n",
      "Epoch 266/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 267/400\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0549\n",
      "Epoch 268/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1267\n",
      "Epoch 269/400\n",
      "12/12 [==============================] - 0s 945us/step - loss: 0.1010\n",
      "Epoch 270/400\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0771\n",
      "Epoch 271/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0750\n",
      "Epoch 272/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1257\n",
      "Epoch 273/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0482\n",
      "Epoch 274/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0869\n",
      "Epoch 275/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1110\n",
      "Epoch 276/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0898\n",
      "Epoch 277/400\n",
      "12/12 [==============================] - 0s 321us/step - loss: 0.0512\n",
      "Epoch 278/400\n",
      "12/12 [==============================] - 0s 397us/step - loss: 0.0942\n",
      "Epoch 279/400\n",
      "12/12 [==============================] - 0s 295us/step - loss: 0.0593\n",
      "Epoch 280/400\n",
      "12/12 [==============================] - 0s 503us/step - loss: 0.0937\n",
      "Epoch 281/400\n",
      "12/12 [==============================] - 0s 607us/step - loss: 0.0355\n",
      "Epoch 282/400\n",
      "12/12 [==============================] - 0s 404us/step - loss: 0.0611\n",
      "Epoch 283/400\n",
      "12/12 [==============================] - 0s 317us/step - loss: 0.1186\n",
      "Epoch 284/400\n",
      "12/12 [==============================] - 0s 243us/step - loss: 0.0851\n",
      "Epoch 285/400\n",
      "12/12 [==============================] - 0s 275us/step - loss: 0.0321\n",
      "Epoch 286/400\n",
      "12/12 [==============================] - 0s 670us/step - loss: 0.1162\n",
      "Epoch 287/400\n",
      "12/12 [==============================] - 0s 731us/step - loss: 0.0510\n",
      "Epoch 288/400\n",
      "12/12 [==============================] - 0s 512us/step - loss: 0.1008\n",
      "Epoch 289/400\n",
      "12/12 [==============================] - 0s 440us/step - loss: 0.0924\n",
      "Epoch 290/400\n",
      "12/12 [==============================] - 0s 807us/step - loss: 0.0545\n",
      "Epoch 291/400\n",
      "12/12 [==============================] - 0s 616us/step - loss: 0.1099\n",
      "Epoch 292/400\n",
      "12/12 [==============================] - 0s 319us/step - loss: 0.0427\n",
      "Epoch 293/400\n",
      "12/12 [==============================] - 0s 556us/step - loss: 0.0846\n",
      "Epoch 294/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1411\n",
      "Epoch 295/400\n",
      "12/12 [==============================] - 0s 986us/step - loss: 0.0677\n",
      "Epoch 296/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/400\n",
      "12/12 [==============================] - 0s 467us/step - loss: 0.0695\n",
      "Epoch 298/400\n",
      "12/12 [==============================] - 0s 591us/step - loss: 0.0455\n",
      "Epoch 299/400\n",
      "12/12 [==============================] - 0s 677us/step - loss: 0.0507\n",
      "Epoch 300/400\n",
      "12/12 [==============================] - 0s 615us/step - loss: 0.0511\n",
      "Epoch 301/400\n",
      "12/12 [==============================] - 0s 553us/step - loss: 0.0586\n",
      "Epoch 302/400\n",
      "12/12 [==============================] - 0s 505us/step - loss: 0.0672\n",
      "Epoch 303/400\n",
      "12/12 [==============================] - 0s 741us/step - loss: 0.1134\n",
      "Epoch 304/400\n",
      "12/12 [==============================] - 0s 631us/step - loss: 0.0618\n",
      "Epoch 305/400\n",
      "12/12 [==============================] - 0s 560us/step - loss: 0.0558\n",
      "Epoch 306/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0743\n",
      "Epoch 307/400\n",
      "12/12 [==============================] - 0s 891us/step - loss: 0.0827\n",
      "Epoch 308/400\n",
      "12/12 [==============================] - 0s 819us/step - loss: 0.0767\n",
      "Epoch 309/400\n",
      "12/12 [==============================] - 0s 597us/step - loss: 0.0500\n",
      "Epoch 310/400\n",
      "12/12 [==============================] - 0s 610us/step - loss: 0.1199\n",
      "Epoch 311/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0486\n",
      "Epoch 312/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.1133\n",
      "Epoch 313/400\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0790\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00313: early stopping\n",
      "best epoch =  213\n",
      "smallest loss = 0.01978403329849243\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 100, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "#historyData = model.fit(xarray,yarray,epochs=500,callbacks=[es])\n",
    "historyData = model.fit(X_train,Y_train,epochs=400,callbacks=[es]) \n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e23556",
   "metadata": {},
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9520a974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row [0] data:  T1=  -10.0 , gam=  870.5882352941176 , qsol=  6.696 , voltage_to_load=  24.851122625215886 , power=  88.53466135458166 ,  predicted voltage_to_load =  24.773911076784135 , predicted power=  81.86267946958542\n",
      "row [0] data:  T1=  -10.0 , gam=  870.5882352941176 , qsol=  8.928 , voltage_to_load=  25.216580310880826 , power=  68.59442231075697 ,  predicted voltage_to_load =  25.527489101886747 , predicted power=  62.53010231256485\n",
      "row [0] data:  T1=  30.0 , gam=  729.4117647058823 , qsol=  6.696 , voltage_to_load=  27.957512953367875 , power=  111.66533864541832 ,  predicted voltage_to_load =  27.85775817632675 , predicted power=  98.01353523135185\n",
      "row [0] data:  T1=  -10.0 , gam=  729.4117647058823 , qsol=  6.696 , voltage_to_load=  24.57702936096718 , power=  86.1418326693227 ,  predicted voltage_to_load =  24.885073113441468 , predicted power=  77.19779433012008\n",
      "row [0] data:  T1=  30.0 , gam=  870.5882352941176 , qsol=  4.464 , voltage_to_load=  27.500690846286698 , power=  162.71235059760954 ,  predicted voltage_to_load =  25.840517991781233 , predicted power=  144.31455001831054\n",
      "row [0] data:  T1=  -10.0 , gam=  729.4117647058823 , qsol=  8.928 , voltage_to_load=  24.94248704663212 , power=  66.99920318725098 ,  predicted voltage_to_load =  25.554564648866652 , predicted power=  59.801859045028685\n",
      "row [0] data:  T1=  -10.0 , gam=  729.4117647058823 , qsol=  4.464 , voltage_to_load=  23.846113989637306 , power=  110.79521912350596 ,  predicted voltage_to_load =  23.392679810523987 , predicted power=  106.68612170219421\n",
      "row [0] data:  T1=  10.0 , gam=  729.4117647058823 , qsol=  4.464 , voltage_to_load=  25.49067357512953 , power=  138.7840637450199 ,  predicted voltage_to_load =  24.787866634130477 , predicted power=  120.10274987220764\n",
      "row [0] data:  T1=  10.0 , gam=  870.5882352941176 , qsol=  4.464 , voltage_to_load=  25.85613126079447 , power=  143.56972111553782 ,  predicted voltage_to_load =  24.50033359527588 , predicted power=  128.36705985069275\n",
      "row [0] data:  T1=  -10.0 , gam=  870.5882352941176 , qsol=  4.464 , voltage_to_load=  24.211571675302242 , power=  125.2247011952191 ,  predicted voltage_to_load =  23.12930256128311 , predicted power=  113.72917609214782\n",
      "row [0] data:  T1=  30.0 , gam=  729.4117647058823 , qsol=  8.928 , voltage_to_load=  28.414335060449048 , power=  86.93944223107569 ,  predicted voltage_to_load =  28.20979388952255 , predicted power=  73.39768282175064\n",
      "row [0] data:  T1=  30.0 , gam=  729.4117647058823 , qsol=  4.464 , voltage_to_load=  27.043868739205525 , power=  157.92669322709162 ,  predicted voltage_to_load =  26.16207597851753 , predicted power=  136.10126490592955\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "Tamed = 10\n",
    "IDmed = 800.\n",
    "RLmed = 6.696\n",
    "VLmed = 26.45\n",
    "Wdmed = 100.1\n",
    "\n",
    "\n",
    "#first point (row [0])comparison of data and prediction\n",
    "# put in a loop to print comparion for all data points\n",
    "for i in range(len(X_train)):\n",
    "    \n",
    "    test = [[ X_train[i][0] , X_train[i][1] , X_train[i][2] ]]\n",
    "    testarray = np.array(test)\n",
    "    outpt = model.predict(testarray)\n",
    "    print ('row [0] data:  T1= ', X_train[i][0]*Tamed, ', gam= ', X_train[i][1]*IDmed, \\\n",
    "        ', qsol= ', X_train[i][2]*RLmed,', voltage_to_load= ', Y_train[i][0]*VLmed,\\\n",
    "        ', power= ', Y_train[i][1]*Wdmed,',  predicted voltage_to_load = ', outpt[0][0]*VLmed,\\\n",
    "           ', predicted power= ', outpt[0][1]*Wdmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cee08",
   "metadata": {},
   "source": [
    "# (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f307f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row [0] data:  T1=  -10.0 , gam=  870.5882352941176 , qsol=  6.696 , voltage_to_load=  24.851122625215886 , power=  88.53466135458166 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  -10.0 , gam=  870.5882352941176 , qsol=  8.928 , voltage_to_load=  25.216580310880826 , power=  68.59442231075697 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  30.0 , gam=  729.4117647058823 , qsol=  6.696 , voltage_to_load=  27.957512953367875 , power=  111.66533864541832 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  -10.0 , gam=  729.4117647058823 , qsol=  6.696 , voltage_to_load=  24.57702936096718 , power=  86.1418326693227 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  30.0 , gam=  870.5882352941176 , qsol=  4.464 , voltage_to_load=  27.500690846286698 , power=  162.71235059760954 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  -10.0 , gam=  729.4117647058823 , qsol=  8.928 , voltage_to_load=  24.94248704663212 , power=  66.99920318725098 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  -10.0 , gam=  729.4117647058823 , qsol=  4.464 , voltage_to_load=  23.846113989637306 , power=  110.79521912350596 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  10.0 , gam=  729.4117647058823 , qsol=  4.464 , voltage_to_load=  25.49067357512953 , power=  138.7840637450199 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  10.0 , gam=  870.5882352941176 , qsol=  4.464 , voltage_to_load=  25.85613126079447 , power=  143.56972111553782 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  -10.0 , gam=  870.5882352941176 , qsol=  4.464 , voltage_to_load=  24.211571675302242 , power=  125.2247011952191 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  30.0 , gam=  729.4117647058823 , qsol=  8.928 , voltage_to_load=  28.414335060449048 , power=  86.93944223107569 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n",
      "row [0] data:  T1=  30.0 , gam=  729.4117647058823 , qsol=  4.464 , voltage_to_load=  27.043868739205525 , power=  157.92669322709162 ,  predicted voltage_to_load =  26.587046632124352 , predicted power=  101.29641434262945\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "Tamed = 10\n",
    "IDmed = 800.\n",
    "RLmed = 6.696\n",
    "VLmed = 26.45\n",
    "Wdmed = 100.1\n",
    "\n",
    "\n",
    "#first point (row [0])comparison of data and prediction\n",
    "# put in a loop to print comparion for all data points\n",
    "for i in range(len(X_train)):\n",
    "    \n",
    "    test = [[ X_train[i][0] , X_train[i][1] , X_train[i][2] ]]\n",
    "    testarray = np.array(test)\n",
    "\n",
    "    print ('row [0] data:  T1= ', X_train[i][0]*Tamed, ', gam= ', X_train[i][1]*IDmed, \\\n",
    "        ', qsol= ', X_train[i][2]*RLmed ,', voltage_to_load= ', Y_train[i][0]*VLmed,\\\n",
    "        ', power= ', Y_train[i][1]*Wdmed,',  predicted voltage_to_load = ', Y_test[0][0]*VLmed,\\\n",
    "           ', predicted power= ', Y_test[0][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84556a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a89bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
