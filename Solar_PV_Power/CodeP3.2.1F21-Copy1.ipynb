{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "600.0\n",
      "27.45\n",
      "55.15\n",
      "166.9\n",
      "[[0.         1.         0.33333333 0.8852459 ]\n",
      " [1.         0.5        0.33333333 0.8852459 ]\n",
      " [2.         0.5        0.33333333 0.8852459 ]\n",
      " [0.         0.5        0.33333333 1.8870674 ]\n",
      " [1.         0.5        0.33333333 1.8870674 ]\n",
      " [2.         0.5        0.33333333 1.8870674 ]\n",
      " [0.         0.5        0.33333333 3.50455373]\n",
      " [1.         0.5        0.33333333 3.50455373]\n",
      " [2.         0.5        0.33333333 3.50455373]\n",
      " [0.         0.5        0.33333333 6.19672131]\n",
      " [1.         0.5        0.33333333 6.19672131]\n",
      " [2.         0.5        0.33333333 6.19672131]\n",
      " [0.         0.5        0.83333333 0.25500911]\n",
      " [1.         0.5        0.83333333 0.25500911]\n",
      " [2.         0.5        0.83333333 0.25500911]\n",
      " [0.         0.5        0.83333333 0.7723133 ]\n",
      " [1.         0.5        0.83333333 0.7723133 ]\n",
      " [2.         0.5        0.83333333 0.7723133 ]\n",
      " [0.         0.5        0.83333333 1.57377049]\n",
      " [1.         0.5        0.83333333 1.57377049]\n",
      " [2.         0.5        0.83333333 1.57377049]\n",
      " [0.         0.5        0.83333333 2.88160291]\n",
      " [1.         0.5        0.83333333 2.88160291]\n",
      " [2.         0.5        0.83333333 2.88160291]\n",
      " [0.         0.5        1.16666667 0.17850638]\n",
      " [1.         0.5        1.16666667 0.17850638]\n",
      " [2.         0.5        1.16666667 0.17850638]\n",
      " [0.         0.5        1.16666667 0.52094718]\n",
      " [1.         0.5        1.16666667 0.52094718]\n",
      " [2.         0.5        1.16666667 0.52094718]\n",
      " [0.         0.5        1.16666667 1.08196721]\n",
      " [1.         0.5        1.16666667 1.08196721]\n",
      " [2.         0.5        1.16666667 1.08196721]\n",
      " [0.         0.5        1.16666667 2.01457195]\n",
      " [1.         0.5        1.16666667 2.01457195]\n",
      " [2.         0.5        1.16666667 2.01457195]\n",
      " [0.         0.5        1.66666667 0.1428051 ]\n",
      " [1.         0.5        1.66666667 0.1428051 ]\n",
      " [2.         0.5        1.66666667 0.1428051 ]\n",
      " [0.         0.5        1.66666667 0.42622951]\n",
      " [1.         0.5        1.66666667 0.42622951]\n",
      " [2.         0.5        1.66666667 0.42622951]\n",
      " [0.         0.5        1.66666667 0.91803279]\n",
      " [1.         0.5        1.66666667 0.91803279]\n",
      " [2.         0.5        1.66666667 0.91803279]\n",
      " [0.         0.5        1.66666667 1.5154827 ]\n",
      " [1.         0.5        1.66666667 1.5154827 ]\n",
      " [2.         0.5        1.66666667 1.5154827 ]\n",
      " [0.         1.         0.33333333 0.8852459 ]\n",
      " [1.         1.         0.33333333 0.8852459 ]\n",
      " [0.         1.         0.33333333 1.8870674 ]\n",
      " [2.         1.         0.33333333 1.8870674 ]\n",
      " [1.         1.         0.33333333 3.50455373]\n",
      " [0.         1.         0.33333333 6.19672131]\n",
      " [2.         1.         0.33333333 6.19672131]\n",
      " [1.         1.         0.83333333 0.25500911]\n",
      " [0.         1.         0.83333333 0.7723133 ]\n",
      " [2.         1.         0.83333333 0.7723133 ]\n",
      " [1.         1.         0.83333333 1.57377049]\n",
      " [0.         1.         0.83333333 2.88160291]\n",
      " [2.         1.         0.83333333 2.88160291]\n",
      " [1.         1.         1.16666667 0.17850638]\n",
      " [0.         1.         1.16666667 0.52094718]\n",
      " [2.         1.         1.16666667 0.52094718]\n",
      " [1.         1.         1.16666667 1.08196721]\n",
      " [0.         1.         1.16666667 2.01457195]\n",
      " [2.         1.         1.16666667 2.01457195]\n",
      " [1.         1.         1.66666667 0.1428051 ]\n",
      " [0.         1.         1.66666667 0.42622951]\n",
      " [2.         1.         1.66666667 0.42622951]\n",
      " [1.         1.         1.66666667 0.91803279]\n",
      " [0.         1.         1.66666667 1.5154827 ]\n",
      " [2.         1.         1.66666667 1.5154827 ]\n",
      " [2.         1.5        0.33333333 0.8852459 ]\n",
      " [1.         1.5        0.33333333 1.8870674 ]\n",
      " [0.         1.5        0.33333333 3.50455373]\n",
      " [2.         1.5        0.33333333 3.50455373]\n",
      " [1.         1.5        0.33333333 6.19672131]\n",
      " [0.         1.5        0.83333333 0.25500911]\n",
      " [2.         1.5        0.83333333 0.25500911]\n",
      " [1.         1.5        0.83333333 0.7723133 ]\n",
      " [0.         1.5        0.83333333 1.57377049]\n",
      " [2.         1.5        0.83333333 1.57377049]\n",
      " [1.         1.5        0.83333333 2.88160291]\n",
      " [0.         1.5        1.16666667 0.17850638]\n",
      " [2.         1.5        1.16666667 0.17850638]\n",
      " [1.         1.5        1.16666667 0.52094718]\n",
      " [0.         1.5        1.16666667 1.08196721]\n",
      " [2.         1.5        1.16666667 1.08196721]\n",
      " [1.         1.5        1.16666667 2.01457195]\n",
      " [0.         1.5        1.66666667 0.1428051 ]\n",
      " [2.         1.5        1.66666667 0.1428051 ]\n",
      " [1.         1.5        1.66666667 0.42622951]\n",
      " [0.         1.5        1.66666667 0.91803279]\n",
      " [2.         1.5        1.66666667 0.91803279]\n",
      " [1.         1.5        1.66666667 1.5154827 ]]\n",
      "[[0.83408885 0.52306771]\n",
      " [0.50589302 0.38466147]\n",
      " [0.50589302 0.19233074]\n",
      " [0.83408885 0.24505692]\n",
      " [1.66999093 0.98202516]\n",
      " [1.07887579 0.4104254 ]\n",
      " [0.83408885 0.13181546]\n",
      " [1.66999093 0.52846016]\n",
      " [2.00543971 0.76213301]\n",
      " [0.83408885 0.07429599]\n",
      " [1.66999093 0.29898143]\n",
      " [3.33998187 1.1959257 ]\n",
      " [0.87760653 2.00898742]\n",
      " [0.37715322 0.74475734]\n",
      " [0.37715322 0.37207909]\n",
      " [0.87760653 0.66327142]\n",
      " [1.75521306 2.65368484]\n",
      " [1.14415231 1.12762133]\n",
      " [0.87760653 0.32534452]\n",
      " [1.75702629 1.30197723]\n",
      " [2.33363554 2.29838226]\n",
      " [0.87760653 0.17735171]\n",
      " [1.75521306 0.71120431]\n",
      " [3.51223935 2.84481726]\n",
      " [0.89392566 2.97243859]\n",
      " [0.36990027 1.02576393]\n",
      " [0.36990027 0.51288197]\n",
      " [0.89392566 1.01797484]\n",
      " [1.78785131 4.07369682]\n",
      " [1.08250227 1.49670461]\n",
      " [0.89392566 0.49011384]\n",
      " [1.78785131 1.96165368]\n",
      " [2.25022665 3.10904733]\n",
      " [0.89401632 0.26303176]\n",
      " [1.78785131 1.05332534]\n",
      " [3.57570263 4.21390054]\n",
      " [0.92112421 3.94787298]\n",
      " [0.41885766 1.63511084]\n",
      " [0.41885766 0.81725584]\n",
      " [0.92112421 1.32234871]\n",
      " [1.84224841 5.29119233]\n",
      " [1.25113327 2.4397843 ]\n",
      " [0.92112421 0.6141402 ]\n",
      " [1.84224841 2.45656081]\n",
      " [2.69446963 5.25584182]\n",
      " [0.92112421 0.37147993]\n",
      " [1.84224841 1.4877172 ]\n",
      " [3.68449683 5.95266627]\n",
      " [0.80077765 0.48089727]\n",
      " [0.50670898 0.38510186]\n",
      " [0.80077765 0.22559467]\n",
      " [1.08014506 0.41045836]\n",
      " [1.6015553  0.4858962 ]\n",
      " [0.80077765 0.06869961]\n",
      " [3.20311061 1.09919376]\n",
      " [0.37824116 0.74491072]\n",
      " [0.84205485 0.60950827]\n",
      " [1.14553037 1.12800767]\n",
      " [1.6841097  1.19644215]\n",
      " [0.84205485 0.16335746]\n",
      " [3.3682194  2.61371939]\n",
      " [0.37138713 1.0259408 ]\n",
      " [0.85691464 0.93578006]\n",
      " [1.08384406 1.49703607]\n",
      " [1.71382928 1.80224308]\n",
      " [0.85691464 0.24198291]\n",
      " [3.42765857 3.87172654]\n",
      " [0.41936537 1.63517316]\n",
      " [0.88333205 1.21533725]\n",
      " [1.25167724 2.44024566]\n",
      " [1.7666641  2.2570549 ]\n",
      " [0.88333205 0.3418136 ]\n",
      " [3.5333282  5.46901764]\n",
      " [0.50670898 0.19255093]\n",
      " [1.53221306 0.82593002]\n",
      " [0.76610653 0.11118289]\n",
      " [2.00598368 0.76227981]\n",
      " [1.53221306 0.25151779]\n",
      " [0.80559655 1.68955315]\n",
      " [0.37824116 0.37245536]\n",
      " [1.61119311 2.23148529]\n",
      " [0.80559655 0.27377019]\n",
      " [2.3342883  2.29858167]\n",
      " [1.61119311 0.59807191]\n",
      " [0.81981296 2.49958656]\n",
      " [0.37138713 0.5129704 ]\n",
      " [1.63962593 3.42600675]\n",
      " [0.81981296 0.4123897 ]\n",
      " [2.25106074 3.10922876]\n",
      " [1.63962593 0.88592941]\n",
      " [0.84508658 3.32009906]\n",
      " [0.41936537 0.81758658]\n",
      " [1.69017316 4.44950028]\n",
      " [0.84508658 0.51645985]\n",
      " [2.69592022 5.25591372]\n",
      " [1.69017316 1.25142195]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP3.2.1F21\n",
    "    V.P. Carey ME249, Fall 2021\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for PV power system'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the following 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# define meadian values of input variables - add your values here\n",
    "#Tamed = 10  #make sure Tamed does not = 0\n",
    "##IDmed = 600\n",
    "#RLmed = 27.45\n",
    "\n",
    "#create input data array\n",
    "xdata = []\n",
    "\n",
    "#Part 1 input data: Mode number, Air temp (degC), ID (W/sqm), load resistance (ohms)\n",
    "xdata = [[0.0, 20.0, 200.0, 24.3]]  \n",
    "xdata.append([1., 10.0, 200.0, 24.3])\n",
    "xdata.append([2., 10.0, 200.0, 24.3] )\n",
    "xdata.append([0., 10.0, 200.0, 51.8] )\n",
    "xdata.append([1., 10.0, 200.0, 51.8] )      \n",
    "xdata.append([2., 10.0, 200.0, 51.8] )\n",
    "xdata.append([0., 10.0, 200.0, 96.2] )\n",
    "xdata.append([1., 10.0, 200.0, 96.2] )\n",
    "xdata.append([2., 10.0, 200.0, 96.2] )\n",
    "xdata.append([0., 10.0, 200.0, 170.1])\n",
    "         \n",
    "xdata.append([1., 10.0, 200.0, 170.1])\n",
    "xdata.append([2., 10.0, 200.0, 170.1])\n",
    "xdata.append([0., 10.0, 500.0, 7.0])\n",
    "xdata.append([1., 10.0, 500.0, 7.0])\n",
    "xdata.append([2., 10.0, 500.0, 7.0])\n",
    "         \n",
    "xdata.append([0., 10.0, 500.0, 21.2])\n",
    "xdata.append([1., 10.0, 500.0, 21.2])\n",
    "xdata.append([2., 10.0, 500.0, 21.2])\n",
    "xdata.append([0., 10.0, 500.0, 43.2])\n",
    "xdata.append([1., 10.0, 500.0, 43.2])\n",
    "         \n",
    "xdata.append([2., 10.0, 500.0, 43.2])\n",
    "xdata.append([0., 10.0, 500.0, 79.1])\n",
    "xdata.append([1., 10.0, 500.0, 79.1])\n",
    "xdata.append([2., 10.0, 500.0, 79.1])\n",
    "xdata.append([0., 10.0, 700.0, 4.9])\n",
    "         \n",
    "xdata.append([1., 10.0, 700.0, 4.9])\n",
    "xdata.append([2., 10.0, 700.0, 4.9])\n",
    "xdata.append([0., 10.0, 700.0, 14.3])\n",
    "xdata.append([1., 10.0, 700.0, 14.3])\n",
    "xdata.append([2., 10.0, 700.0, 14.3])\n",
    "         \n",
    "xdata.append([0., 10.0, 700.0, 29.7])\n",
    "xdata.append([1., 10.0, 700.0, 29.7])\n",
    "xdata.append([2., 10.0, 700.0, 29.7])\n",
    "xdata.append([0., 10.0, 700.0, 55.3])\n",
    "xdata.append([1., 10.0, 700.0, 55.3])\n",
    "xdata.append([2., 10.0, 700.0, 55.3])\n",
    "         \n",
    "xdata.append([0., 10.0, 1000.0, 3.92])\n",
    "xdata.append([1., 10.0, 1000.0, 3.92])\n",
    "xdata.append([2., 10.0, 1000.0, 3.92])\n",
    "xdata.append([0., 10.0, 1000.0, 11.7])\n",
    "xdata.append([1., 10.0, 1000.0, 11.7])\n",
    "         \n",
    "xdata.append([2., 10.0, 1000.0, 11.7])\n",
    "xdata.append([0., 10.0, 1000.0, 25.2])\n",
    "xdata.append([1., 10.0, 1000.0, 25.2])\n",
    "xdata.append([2., 10.0, 1000.0, 25.2])\n",
    "xdata.append([0., 10.0, 1000.0, 41.6])\n",
    "xdata.append([1., 10.0, 1000.0, 41.6])\n",
    "xdata.append([2., 10.0, 1000.0, 41.6])\n",
    "\n",
    "xdata.append([0.0, 20.0, 200.0, 24.3])\n",
    "xdata.append([1.0, 20.0, 200.0, 24.3])\n",
    "xdata.append([0.0, 20.0, 200.0, 51.8])\n",
    "xdata.append([2.0, 20.0, 200.0, 51.8])\n",
    "xdata.append([1.0, 20.0, 200.0, 96.2]) \n",
    "xdata.append([0.0, 20.0, 200.0, 170.1]) \n",
    "xdata.append([2.0, 20.0, 200.0, 170.1])  \n",
    "xdata.append([1.0, 20.0, 500.0, 7.0]) \n",
    "xdata.append([0.0, 20.0, 500.0, 21.2]) \n",
    "xdata.append([2.0, 20.0, 500.0, 21.2]) \n",
    "xdata.append([1.0, 20.0, 500.0, 43.2]) \n",
    "xdata.append([0.0, 20.0, 500.0, 79.1]) \n",
    "xdata.append([2.0, 20.0, 500.0, 79.1]) \n",
    "xdata.append([1.0, 20.0, 700.0, 4.9]) \n",
    "xdata.append([0.0, 20.0, 700.0, 14.3])\n",
    "xdata.append([2.0, 20.0, 700.0, 14.3]) \n",
    "xdata.append([1.0, 20.0, 700.0, 29.7]) \n",
    "xdata.append([0.0, 20.0, 700.0, 55.3]) \n",
    "xdata.append([2.0, 20.0, 700.0, 55.3]) \n",
    "xdata.append([1.0, 20.0, 1000.0, 3.92]) \n",
    "xdata.append([0.0, 20.0, 1000.0, 11.7]) \n",
    "xdata.append([2.0, 20.0, 1000.0, 11.7]) \n",
    "xdata.append([1.0, 20.0, 1000.0, 25.2]) \n",
    "xdata.append([0.0, 20.0, 1000.0, 41.6]) \n",
    "xdata.append([2.0, 20.0, 1000.0, 41.6])\n",
    "xdata.append([2.0, 30.0, 200.0, 24.3]) \n",
    "xdata.append([1.0, 30.0, 200.0, 51.8]) \n",
    "xdata.append([0.0, 30.0, 200.0, 96.2]) \n",
    "xdata.append([2.0, 30.0, 200.0, 96.2])\n",
    "xdata.append([1.0, 30.0, 200.0, 170.1]) \n",
    "xdata.append([0.0, 30.0, 500.0, 7.0]) \n",
    "xdata.append([2.0, 30.0, 500.0, 7.0])\n",
    "xdata.append([1.0, 30.0, 500.0, 21.2]) \n",
    "xdata.append([0.0, 30.0, 500.0, 43.2]) \n",
    "xdata.append([2.0, 30.0, 500.0, 43.2]) \n",
    "xdata.append([1.0, 30.0, 500.0, 79.1])\n",
    "xdata.append([0.0, 30.0, 700.0, 4.9]) \n",
    "xdata.append([2.0, 30.0, 700.0, 4.9]) \n",
    "xdata.append([1.0, 30.0, 700.0, 14.3]) \n",
    "xdata.append([0.0, 30.0, 700.0, 29.7]) \n",
    "xdata.append([2.0, 30.0, 700.0, 29.7]) \n",
    "xdata.append([1.0, 30.0, 700.0, 55.3]) \n",
    "xdata.append([0.0, 30.0, 1000.0, 3.92]) \n",
    "xdata.append([2.0, 30.0, 1000.0, 3.92]) \n",
    "xdata.append([1.0, 30.0, 1000.0, 11.7]) \n",
    "xdata.append([0.0, 30.0, 1000.0, 25.2]) \n",
    "xdata.append([2.0, 30.0, 1000.0, 25.2]) \n",
    "xdata.append([1.0, 30.0, 1000.0, 41.6]) \n",
    "\n",
    "\n",
    "\n",
    "''' ADD THE REST OF THE INPUT DATA FROM DS3.2.1multiModePerfF21 AND CONVERT TO:\n",
    "xdata = [[0., 10.0/Tamed, 200.0/IDmed, 24.3/RLmed]] \n",
    "xdata.append([1., 10.0/Tamed, 200.0/IDmed, 24.3/RLmed]) \n",
    "xdata.append([2., 10.0/Tamed, 200.0/IDmed, 24.3/RLmed])\n",
    "... etc.'''\n",
    "\n",
    "\n",
    "# define meadian values of input variables - add your values here\n",
    "#VLmed = 55.15\n",
    "#Wdmed = 170.55\n",
    "\n",
    "#create input data array\n",
    "ydata = []\n",
    "\n",
    "#Part 1 output data: VL (V) and Power out Wd (W)\n",
    "ydata = [[46.0, 87.3]] \n",
    "ydata.append([27.9, 64.2]) \n",
    "ydata.append([27.9, 32.1]) \n",
    "ydata.append([46.0, 40.9]) \n",
    "ydata.append([92.1, 163.9]) \n",
    "ydata.append([59.5, 68.5])\n",
    "ydata.append([46.0, 22.0])\n",
    "ydata.append([92.1, 88.2]) \n",
    "ydata.append([110.6, 127.2]) \n",
    "ydata.append([46.0, 12.4])\n",
    "ydata.append([92.1, 49.9])\n",
    "ydata.append([184.2, 199.6]) \n",
    "ydata.append([48.4, 335.3]) \n",
    "ydata.append([20.8, 124.3]) \n",
    "ydata.append([20.8, 62.1]) \n",
    "ydata.append([48.4, 110.7]) \n",
    "ydata.append([96.8, 442.9]) \n",
    "ydata.append([63.1, 188.2]) \n",
    "ydata.append([48.4, 54.3]) \n",
    "ydata.append([96.9, 217.3]) \n",
    "ydata.append([128.7, 383.6]) \n",
    "ydata.append([48.4, 29.6]) \n",
    "ydata.append([96.8, 118.7]) \n",
    "ydata.append([193.7, 474.8]) \n",
    "ydata.append([49.3, 496.1]) \n",
    "ydata.append([20.4, 171.2]) \n",
    "ydata.append([20.4, 85.6]) \n",
    "ydata.append([49.3, 169.9]) \n",
    "ydata.append([98.6, 679.9]) \n",
    "ydata.append([59.7, 249.8]) \n",
    "ydata.append([49.3, 81.8]) \n",
    "ydata.append([98.6, 327.4]) \n",
    "ydata.append([124.1, 518.9]) \n",
    "ydata.append([49.305, 43.9]) \n",
    "ydata.append([98.6, 175.8]) \n",
    "ydata.append([197.2, 703.3]) \n",
    "ydata.append([50.8, 658.9]) \n",
    "ydata.append([23.1, 272.9]) \n",
    "ydata.append([23.1, 136.4]) \n",
    "ydata.append([50.8, 220.7]) \n",
    "ydata.append([101.6, 883.1]) \n",
    "ydata.append([69.0, 407.2]) \n",
    "ydata.append([50.8, 102.5]) \n",
    "ydata.append([101.6, 410.0]) \n",
    "ydata.append([148.6, 877.2]) \n",
    "ydata.append([50.8, 62.0],)\n",
    "ydata.append([101.6, 248.3]) \n",
    "ydata.append([203.2, 993.5])\n",
    "ydata.append([44.162887500000004, 80.26175441718752])\n",
    "ydata.append([27.945, 64.2735]) \n",
    "ydata.append([44.162887500000004, 37.65174965902])\n",
    "ydata.append([59.56999999999999, 68.505499999998]) \n",
    "ydata.append([88.32577500000001, 81.096076188675]) \n",
    "ydata.append([44.162887500000004, 11.465964916776]) \n",
    "ydata.append([176.65155000000001, 183.45543866772]) \n",
    "ydata.append([20.86, 124.3256]) \n",
    "ydata.append([46.439325, 101.72692954979362]) \n",
    "ydata.append([63.175999999999995, 188.26448]) \n",
    "ydata.append([92.87865, 199.68619504218745])\n",
    "ydata.append([46.439325, 27.264360385026865]) \n",
    "ydata.append([185.7573, 436.22976616042985]) \n",
    "ydata.append([20.482, 171.22951999999998]) \n",
    "ydata.append([47.2588425, 156.18169191886756]) \n",
    "ydata.append([59.774, 249.85531999999998])\n",
    "ydata.append([94.517685, 300.7943696215227]) \n",
    "ydata.append([47.2588425, 40.38694745822434]) \n",
    "ydata.append([189.03537, 646.1911593315895])\n",
    "ydata.append([23.128, 272.91040000000004]) \n",
    "ydata.append([48.7157625, 202.8397876885817]) \n",
    "ydata.append([69.03, 407.27700000000004]) \n",
    "ydata.append([97.431525, 376.7024628502232]) \n",
    "ydata.append([48.7157625, 57.0486902874136])\n",
    "ydata.append([194.86305, 912.7790445986176])\n",
    "ydata.append([27.945, 32.13675]) \n",
    "ydata.append([84.50155, 137.84772108885133]) \n",
    "ydata.append([42.250775, 18.556423992729986]) \n",
    "ydata.append([110.63, 127.22449999999999]) \n",
    "ydata.append([84.50155, 41.978318356275715]) \n",
    "ydata.append([44.42865, 281.98642011749996]) \n",
    "ydata.append([20.86, 62.1628])  \n",
    "ydata.append([88.8573, 372.4348944948113]) \n",
    "ydata.append([44.42865, 45.69224400052082]) \n",
    "ydata.append([128.73600000000002, 383.6332807])  \n",
    "ydata.append([88.8573, 99.81820181150441]) \n",
    "ydata.append([45.212685, 417.18099692025]) \n",
    "ydata.append([20.482, 85.61475999999999]) \n",
    "ydata.append([90.42537, 571.8005272473356])\n",
    "ydata.append([45.212685, 68.82784124273485]) \n",
    "ydata.append([124.14599999999999, 518.9302799]) \n",
    "ydata.append([90.42537, 147.86161916160762]) \n",
    "ydata.append([46.606525, 554.1245338203124])\n",
    "ydata.append([23.128, 136.45520000000002]) \n",
    "ydata.append([93.21305, 742.6215974617521])\n",
    "ydata.append([46.606525, 86.19714970538195]) \n",
    "ydata.append([148.68, 877.2120000000001]) \n",
    "ydata.append([93.21305, 208.86232428611777]) \n",
    "\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "yarray= np.array(ydata)\n",
    "Tamed= np.median(xarray[:,1])\n",
    "IDmed= np.median(xarray[:,2])\n",
    "RLmed= np.median(xarray[:,3])\n",
    "VLmed= np.median(yarray[:,0])\n",
    "Wdmed= np.median(yarray[:,1])\n",
    "\n",
    "\n",
    "\n",
    "Xmedian = [1,Tamed,IDmed,RLmed]\n",
    "Ymedian = [VLmed ,Wdmed]\n",
    "X_nor=xarray/Xmedian\n",
    "Y_nor=yarray/Ymedian\n",
    "print(Tamed)\n",
    "print(IDmed)\n",
    "print(RLmed)\n",
    "print(VLmed)\n",
    "print(Wdmed)\n",
    "print(X_nor)\n",
    "print(Y_nor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_nor, Y_nor, test_size=1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 19:02:43.069566: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 19:02:43.069778: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# define neural network model\n",
    "\n",
    "#As seen below, we have created four dense layers. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case. \n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 0.5\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=K.elu, input_shape=[4],  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu,  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "    keras.layers.Dense(2,  kernel_initializer=initializer)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.00150)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 1.6382\n",
      "Epoch 2/800\n",
      "72/72 [==============================] - 0s 189us/step - loss: 0.9847\n",
      "Epoch 3/800\n",
      "72/72 [==============================] - 0s 297us/step - loss: 0.9617\n",
      "Epoch 4/800\n",
      "72/72 [==============================] - 0s 213us/step - loss: 0.9389\n",
      "Epoch 5/800\n",
      "72/72 [==============================] - 0s 322us/step - loss: 0.8826\n",
      "Epoch 6/800\n",
      "72/72 [==============================] - 0s 280us/step - loss: 0.9410\n",
      "Epoch 7/800\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.8902\n",
      "Epoch 8/800\n",
      "72/72 [==============================] - 0s 411us/step - loss: 1.0270\n",
      "Epoch 9/800\n",
      "72/72 [==============================] - 0s 743us/step - loss: 0.8265\n",
      "Epoch 10/800\n",
      "72/72 [==============================] - 0s 366us/step - loss: 0.9120\n",
      "Epoch 11/800\n",
      "72/72 [==============================] - 0s 395us/step - loss: 0.8199\n",
      "Epoch 12/800\n",
      "72/72 [==============================] - 0s 704us/step - loss: 0.9069\n",
      "Epoch 13/800\n",
      "72/72 [==============================] - 0s 463us/step - loss: 0.8377\n",
      "Epoch 14/800\n",
      "72/72 [==============================] - 0s 377us/step - loss: 0.8179\n",
      "Epoch 15/800\n",
      "72/72 [==============================] - 0s 352us/step - loss: 0.8133\n",
      "Epoch 16/800\n",
      "72/72 [==============================] - 0s 203us/step - loss: 0.8171\n",
      "Epoch 17/800\n",
      "72/72 [==============================] - 0s 427us/step - loss: 0.7870\n",
      "Epoch 18/800\n",
      "72/72 [==============================] - 0s 409us/step - loss: 0.8865\n",
      "Epoch 19/800\n",
      "72/72 [==============================] - 0s 309us/step - loss: 0.8472\n",
      "Epoch 20/800\n",
      "72/72 [==============================] - 0s 305us/step - loss: 0.7712\n",
      "Epoch 21/800\n",
      "72/72 [==============================] - 0s 266us/step - loss: 0.7618\n",
      "Epoch 22/800\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.9841\n",
      "Epoch 23/800\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.7648\n",
      "Epoch 24/800\n",
      "72/72 [==============================] - 0s 442us/step - loss: 0.8336\n",
      "Epoch 25/800\n",
      "72/72 [==============================] - 0s 437us/step - loss: 0.7896\n",
      "Epoch 26/800\n",
      "72/72 [==============================] - 0s 431us/step - loss: 0.9187\n",
      "Epoch 27/800\n",
      "72/72 [==============================] - 0s 407us/step - loss: 0.8015\n",
      "Epoch 28/800\n",
      "72/72 [==============================] - 0s 304us/step - loss: 0.7554\n",
      "Epoch 29/800\n",
      "72/72 [==============================] - 0s 545us/step - loss: 0.9949\n",
      "Epoch 30/800\n",
      "72/72 [==============================] - 0s 554us/step - loss: 0.7786\n",
      "Epoch 31/800\n",
      "72/72 [==============================] - 0s 399us/step - loss: 0.7838\n",
      "Epoch 32/800\n",
      "72/72 [==============================] - 0s 539us/step - loss: 0.7499\n",
      "Epoch 33/800\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.7509\n",
      "Epoch 34/800\n",
      "72/72 [==============================] - 0s 434us/step - loss: 0.7632\n",
      "Epoch 35/800\n",
      "72/72 [==============================] - 0s 547us/step - loss: 0.7352\n",
      "Epoch 36/800\n",
      "72/72 [==============================] - 0s 420us/step - loss: 0.8594\n",
      "Epoch 37/800\n",
      "72/72 [==============================] - 0s 307us/step - loss: 0.8005\n",
      "Epoch 38/800\n",
      "72/72 [==============================] - 0s 202us/step - loss: 0.7577\n",
      "Epoch 39/800\n",
      "72/72 [==============================] - 0s 250us/step - loss: 0.7651\n",
      "Epoch 40/800\n",
      "72/72 [==============================] - 0s 352us/step - loss: 0.7951\n",
      "Epoch 41/800\n",
      "72/72 [==============================] - 0s 477us/step - loss: 0.8665\n",
      "Epoch 42/800\n",
      "72/72 [==============================] - 0s 474us/step - loss: 0.7685\n",
      "Epoch 43/800\n",
      "72/72 [==============================] - 0s 442us/step - loss: 0.8934\n",
      "Epoch 44/800\n",
      "72/72 [==============================] - 0s 603us/step - loss: 0.7424\n",
      "Epoch 45/800\n",
      "72/72 [==============================] - 0s 613us/step - loss: 0.7618\n",
      "Epoch 46/800\n",
      "72/72 [==============================] - 0s 432us/step - loss: 0.8209\n",
      "Epoch 47/800\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.7172\n",
      "Epoch 48/800\n",
      "72/72 [==============================] - 0s 434us/step - loss: 0.7133\n",
      "Epoch 49/800\n",
      "72/72 [==============================] - 0s 460us/step - loss: 0.8419\n",
      "Epoch 50/800\n",
      "72/72 [==============================] - 0s 558us/step - loss: 0.7004\n",
      "Epoch 51/800\n",
      "72/72 [==============================] - 0s 570us/step - loss: 0.7119\n",
      "Epoch 52/800\n",
      "72/72 [==============================] - 0s 442us/step - loss: 0.7208\n",
      "Epoch 53/800\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.6817\n",
      "Epoch 54/800\n",
      "72/72 [==============================] - 0s 628us/step - loss: 0.7586\n",
      "Epoch 55/800\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.7844\n",
      "Epoch 56/800\n",
      "72/72 [==============================] - 0s 691us/step - loss: 0.6788\n",
      "Epoch 57/800\n",
      "72/72 [==============================] - 0s 407us/step - loss: 0.7442\n",
      "Epoch 58/800\n",
      "72/72 [==============================] - 0s 387us/step - loss: 0.7055\n",
      "Epoch 59/800\n",
      "72/72 [==============================] - 0s 539us/step - loss: 0.6993\n",
      "Epoch 60/800\n",
      "72/72 [==============================] - 0s 451us/step - loss: 0.7003\n",
      "Epoch 61/800\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.8792\n",
      "Epoch 62/800\n",
      "72/72 [==============================] - 0s 561us/step - loss: 0.6919\n",
      "Epoch 63/800\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.7008\n",
      "Epoch 64/800\n",
      "72/72 [==============================] - 0s 378us/step - loss: 0.6820\n",
      "Epoch 65/800\n",
      "72/72 [==============================] - 0s 299us/step - loss: 0.7199\n",
      "Epoch 66/800\n",
      "72/72 [==============================] - 0s 287us/step - loss: 0.6718\n",
      "Epoch 67/800\n",
      "72/72 [==============================] - 0s 441us/step - loss: 0.7593\n",
      "Epoch 68/800\n",
      "72/72 [==============================] - 0s 342us/step - loss: 0.7006\n",
      "Epoch 69/800\n",
      "72/72 [==============================] - 0s 395us/step - loss: 0.7312\n",
      "Epoch 70/800\n",
      "72/72 [==============================] - 0s 370us/step - loss: 0.7395\n",
      "Epoch 71/800\n",
      "72/72 [==============================] - 0s 409us/step - loss: 0.6767\n",
      "Epoch 72/800\n",
      "72/72 [==============================] - 0s 575us/step - loss: 0.6984\n",
      "Epoch 73/800\n",
      "72/72 [==============================] - 0s 360us/step - loss: 0.6954\n",
      "Epoch 74/800\n",
      "72/72 [==============================] - 0s 489us/step - loss: 0.6894\n",
      "Epoch 75/800\n",
      "72/72 [==============================] - 0s 357us/step - loss: 0.8962\n",
      "Epoch 76/800\n",
      "72/72 [==============================] - 0s 338us/step - loss: 0.6551\n",
      "Epoch 77/800\n",
      "72/72 [==============================] - 0s 302us/step - loss: 0.6688\n",
      "Epoch 78/800\n",
      "72/72 [==============================] - 0s 683us/step - loss: 0.6501\n",
      "Epoch 79/800\n",
      "72/72 [==============================] - 0s 303us/step - loss: 0.6865\n",
      "Epoch 80/800\n",
      "72/72 [==============================] - 0s 353us/step - loss: 0.6936\n",
      "Epoch 81/800\n",
      "72/72 [==============================] - 0s 470us/step - loss: 0.6933\n",
      "Epoch 82/800\n",
      "72/72 [==============================] - 0s 411us/step - loss: 0.6480\n",
      "Epoch 83/800\n",
      "72/72 [==============================] - 0s 321us/step - loss: 0.7050\n",
      "Epoch 84/800\n",
      "72/72 [==============================] - 0s 295us/step - loss: 0.7203\n",
      "Epoch 85/800\n",
      "72/72 [==============================] - 0s 850us/step - loss: 0.7019\n",
      "Epoch 86/800\n",
      "72/72 [==============================] - 0s 275us/step - loss: 0.6287\n",
      "Epoch 87/800\n",
      "72/72 [==============================] - 0s 475us/step - loss: 0.7025\n",
      "Epoch 88/800\n",
      "72/72 [==============================] - 0s 419us/step - loss: 0.6617\n",
      "Epoch 89/800\n",
      "72/72 [==============================] - 0s 299us/step - loss: 0.6345\n",
      "Epoch 90/800\n",
      "72/72 [==============================] - 0s 382us/step - loss: 0.5993\n",
      "Epoch 91/800\n",
      "72/72 [==============================] - 0s 563us/step - loss: 0.6739\n",
      "Epoch 92/800\n",
      "72/72 [==============================] - 0s 359us/step - loss: 0.6093\n",
      "Epoch 93/800\n",
      "72/72 [==============================] - 0s 433us/step - loss: 0.6853\n",
      "Epoch 94/800\n",
      "72/72 [==============================] - 0s 587us/step - loss: 0.5976\n",
      "Epoch 95/800\n",
      "72/72 [==============================] - 0s 714us/step - loss: 0.6684\n",
      "Epoch 96/800\n",
      "72/72 [==============================] - 0s 480us/step - loss: 0.8125\n",
      "Epoch 97/800\n",
      "72/72 [==============================] - 0s 467us/step - loss: 0.6184\n",
      "Epoch 98/800\n",
      "72/72 [==============================] - 0s 297us/step - loss: 0.5618\n",
      "Epoch 99/800\n",
      "72/72 [==============================] - 0s 252us/step - loss: 0.6474\n",
      "Epoch 100/800\n",
      "72/72 [==============================] - 0s 269us/step - loss: 0.5506\n",
      "Epoch 101/800\n",
      "72/72 [==============================] - 0s 339us/step - loss: 0.6510\n",
      "Epoch 102/800\n",
      "72/72 [==============================] - 0s 322us/step - loss: 0.5273\n",
      "Epoch 103/800\n",
      "72/72 [==============================] - 0s 302us/step - loss: 0.5546\n",
      "Epoch 104/800\n",
      "72/72 [==============================] - 0s 326us/step - loss: 0.5744\n",
      "Epoch 105/800\n",
      "72/72 [==============================] - 0s 258us/step - loss: 0.6191\n",
      "Epoch 106/800\n",
      "72/72 [==============================] - 0s 490us/step - loss: 0.5546\n",
      "Epoch 107/800\n",
      "72/72 [==============================] - 0s 297us/step - loss: 0.5572\n",
      "Epoch 108/800\n",
      "72/72 [==============================] - 0s 753us/step - loss: 0.5323\n",
      "Epoch 109/800\n",
      "72/72 [==============================] - 0s 470us/step - loss: 0.7058\n",
      "Epoch 110/800\n",
      "72/72 [==============================] - 0s 322us/step - loss: 0.5548\n",
      "Epoch 111/800\n",
      "72/72 [==============================] - 0s 548us/step - loss: 0.5149\n",
      "Epoch 112/800\n",
      "72/72 [==============================] - 0s 566us/step - loss: 0.4660\n",
      "Epoch 113/800\n",
      "72/72 [==============================] - 0s 393us/step - loss: 0.5347\n",
      "Epoch 114/800\n",
      "72/72 [==============================] - 0s 367us/step - loss: 0.4850\n",
      "Epoch 115/800\n",
      "72/72 [==============================] - 0s 446us/step - loss: 0.4798\n",
      "Epoch 116/800\n",
      "72/72 [==============================] - 0s 212us/step - loss: 0.5040\n",
      "Epoch 117/800\n",
      "72/72 [==============================] - 0s 233us/step - loss: 0.4243\n",
      "Epoch 118/800\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.5312\n",
      "Epoch 119/800\n",
      "72/72 [==============================] - 0s 367us/step - loss: 0.5296\n",
      "Epoch 120/800\n",
      "72/72 [==============================] - 0s 333us/step - loss: 0.5019\n",
      "Epoch 121/800\n",
      "72/72 [==============================] - 0s 287us/step - loss: 0.4266\n",
      "Epoch 122/800\n",
      "72/72 [==============================] - 0s 446us/step - loss: 0.5984\n",
      "Epoch 123/800\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.4274\n",
      "Epoch 124/800\n",
      "72/72 [==============================] - 0s 297us/step - loss: 0.5177\n",
      "Epoch 125/800\n",
      "72/72 [==============================] - 0s 299us/step - loss: 0.5146\n",
      "Epoch 126/800\n",
      "72/72 [==============================] - 0s 387us/step - loss: 0.6350\n",
      "Epoch 127/800\n",
      "72/72 [==============================] - 0s 454us/step - loss: 0.5112\n",
      "Epoch 128/800\n",
      "72/72 [==============================] - 0s 307us/step - loss: 0.4473\n",
      "Epoch 129/800\n",
      "72/72 [==============================] - 0s 387us/step - loss: 0.3985\n",
      "Epoch 130/800\n",
      "72/72 [==============================] - 0s 268us/step - loss: 0.5105\n",
      "Epoch 131/800\n",
      "72/72 [==============================] - 0s 239us/step - loss: 0.4374\n",
      "Epoch 132/800\n",
      "72/72 [==============================] - 0s 267us/step - loss: 0.4282\n",
      "Epoch 133/800\n",
      "72/72 [==============================] - 0s 226us/step - loss: 0.4967\n",
      "Epoch 134/800\n",
      "72/72 [==============================] - 0s 298us/step - loss: 0.6005\n",
      "Epoch 135/800\n",
      "72/72 [==============================] - 0s 395us/step - loss: 0.4099\n",
      "Epoch 136/800\n",
      "72/72 [==============================] - 0s 420us/step - loss: 0.4283\n",
      "Epoch 137/800\n",
      "72/72 [==============================] - 0s 302us/step - loss: 0.4915\n",
      "Epoch 138/800\n",
      "72/72 [==============================] - 0s 259us/step - loss: 0.5363\n",
      "Epoch 139/800\n",
      "72/72 [==============================] - 0s 295us/step - loss: 0.4982\n",
      "Epoch 140/800\n",
      "72/72 [==============================] - 0s 480us/step - loss: 0.4937\n",
      "Epoch 141/800\n",
      "72/72 [==============================] - 0s 282us/step - loss: 0.4566\n",
      "Epoch 142/800\n",
      "72/72 [==============================] - 0s 583us/step - loss: 0.6199\n",
      "Epoch 143/800\n",
      "72/72 [==============================] - 0s 552us/step - loss: 0.5641\n",
      "Epoch 144/800\n",
      "72/72 [==============================] - 0s 374us/step - loss: 0.4636\n",
      "Epoch 145/800\n",
      "72/72 [==============================] - 0s 168us/step - loss: 0.5417\n",
      "Epoch 146/800\n",
      "72/72 [==============================] - 0s 189us/step - loss: 0.4848\n",
      "Epoch 147/800\n",
      "72/72 [==============================] - 0s 173us/step - loss: 0.4190\n",
      "Epoch 148/800\n",
      "72/72 [==============================] - 0s 168us/step - loss: 0.5697\n",
      "Epoch 149/800\n",
      "72/72 [==============================] - 0s 202us/step - loss: 0.5382\n",
      "Epoch 150/800\n",
      "72/72 [==============================] - 0s 184us/step - loss: 0.4825\n",
      "Epoch 151/800\n",
      "72/72 [==============================] - 0s 198us/step - loss: 0.4289\n",
      "Epoch 152/800\n",
      "72/72 [==============================] - 0s 202us/step - loss: 0.4482\n",
      "Epoch 153/800\n",
      "72/72 [==============================] - 0s 334us/step - loss: 0.4271\n",
      "Epoch 154/800\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.5418\n",
      "Epoch 155/800\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.4408\n",
      "Epoch 156/800\n",
      "72/72 [==============================] - 0s 417us/step - loss: 0.4093\n",
      "Epoch 157/800\n",
      "72/72 [==============================] - 0s 731us/step - loss: 0.5678\n",
      "Epoch 158/800\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.556 - 0s 513us/step - loss: 0.5043\n",
      "Epoch 159/800\n",
      "72/72 [==============================] - 0s 443us/step - loss: 0.4504\n",
      "Epoch 160/800\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.4355\n",
      "Epoch 161/800\n",
      "72/72 [==============================] - 0s 310us/step - loss: 0.4109\n",
      "Epoch 162/800\n",
      "72/72 [==============================] - 0s 215us/step - loss: 0.4426\n",
      "Epoch 163/800\n",
      "72/72 [==============================] - 0s 237us/step - loss: 0.5012\n",
      "Epoch 164/800\n",
      "72/72 [==============================] - 0s 376us/step - loss: 0.4464\n",
      "Epoch 165/800\n",
      "72/72 [==============================] - 0s 325us/step - loss: 0.4358\n",
      "Epoch 166/800\n",
      "72/72 [==============================] - 0s 260us/step - loss: 0.4223\n",
      "Epoch 167/800\n",
      "72/72 [==============================] - 0s 329us/step - loss: 0.4459\n",
      "Epoch 168/800\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4233\n",
      "Epoch 169/800\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.5062\n",
      "Epoch 170/800\n",
      "72/72 [==============================] - 0s 393us/step - loss: 0.3859\n",
      "Epoch 171/800\n",
      "72/72 [==============================] - 0s 426us/step - loss: 0.4851\n",
      "Epoch 172/800\n",
      "72/72 [==============================] - 0s 395us/step - loss: 0.4228\n",
      "Epoch 173/800\n",
      "72/72 [==============================] - 0s 228us/step - loss: 0.4814\n",
      "Epoch 174/800\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.4788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python3-7_Keras_env/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.164827). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 297us/step - loss: 0.4454\n",
      "Epoch 175/800\n",
      "72/72 [==============================] - 0s 689us/step - loss: 0.4290\n",
      "Epoch 176/800\n",
      "72/72 [==============================] - 0s 629us/step - loss: 0.4117\n",
      "Epoch 177/800\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5674\n",
      "Epoch 178/800\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4068\n",
      "Epoch 179/800\n",
      "72/72 [==============================] - 0s 388us/step - loss: 0.3786\n",
      "Epoch 180/800\n",
      "72/72 [==============================] - 0s 319us/step - loss: 0.4085\n",
      "Epoch 181/800\n",
      "72/72 [==============================] - 0s 366us/step - loss: 0.4029\n",
      "Epoch 182/800\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.4388\n",
      "Epoch 183/800\n",
      "72/72 [==============================] - 0s 339us/step - loss: 0.4503\n",
      "Epoch 184/800\n",
      "72/72 [==============================] - 0s 574us/step - loss: 0.4400\n",
      "Epoch 185/800\n",
      "72/72 [==============================] - 0s 796us/step - loss: 0.3895\n",
      "Epoch 186/800\n",
      "72/72 [==============================] - 0s 669us/step - loss: 0.5267\n",
      "Epoch 187/800\n",
      "72/72 [==============================] - 0s 932us/step - loss: 0.4322\n",
      "Epoch 188/800\n",
      "72/72 [==============================] - 0s 843us/step - loss: 0.4669\n",
      "Epoch 189/800\n",
      "72/72 [==============================] - 0s 407us/step - loss: 0.3851\n",
      "Epoch 190/800\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5301\n",
      "Epoch 191/800\n",
      "72/72 [==============================] - 0s 200us/step - loss: 0.4208\n",
      "Epoch 192/800\n",
      "72/72 [==============================] - 0s 233us/step - loss: 0.4736\n",
      "Epoch 193/800\n",
      "72/72 [==============================] - 0s 228us/step - loss: 0.4026\n",
      "Epoch 194/800\n",
      "72/72 [==============================] - 0s 252us/step - loss: 0.4512\n",
      "Epoch 195/800\n",
      "72/72 [==============================] - 0s 328us/step - loss: 0.4540\n",
      "Epoch 196/800\n",
      "72/72 [==============================] - 0s 247us/step - loss: 0.4495\n",
      "Epoch 197/800\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4196\n",
      "Epoch 198/800\n",
      "72/72 [==============================] - 0s 385us/step - loss: 0.3993\n",
      "Epoch 199/800\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.468 - 0s 668us/step - loss: 0.4748\n",
      "Epoch 200/800\n",
      "72/72 [==============================] - 0s 313us/step - loss: 0.4441\n",
      "Epoch 201/800\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5928\n",
      "Epoch 202/800\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.3452\n",
      "Epoch 203/800\n",
      "72/72 [==============================] - 0s 226us/step - loss: 0.4114\n",
      "Epoch 204/800\n",
      "72/72 [==============================] - 0s 391us/step - loss: 0.4049\n",
      "Epoch 205/800\n",
      "72/72 [==============================] - 0s 236us/step - loss: 0.4386\n",
      "Epoch 206/800\n",
      "72/72 [==============================] - 0s 229us/step - loss: 0.4885\n",
      "Epoch 207/800\n",
      "72/72 [==============================] - 0s 239us/step - loss: 0.4120\n",
      "Epoch 208/800\n",
      "72/72 [==============================] - 0s 462us/step - loss: 0.3860\n",
      "Epoch 209/800\n",
      "72/72 [==============================] - 0s 483us/step - loss: 0.5438\n",
      "Epoch 210/800\n",
      "72/72 [==============================] - 0s 360us/step - loss: 0.3365\n",
      "Epoch 211/800\n",
      "72/72 [==============================] - 0s 441us/step - loss: 0.5276\n",
      "Epoch 212/800\n",
      "72/72 [==============================] - 0s 321us/step - loss: 0.4668\n",
      "Epoch 213/800\n",
      "72/72 [==============================] - 0s 373us/step - loss: 0.3497\n",
      "Epoch 214/800\n",
      "72/72 [==============================] - 0s 212us/step - loss: 0.4523\n",
      "Epoch 215/800\n",
      "72/72 [==============================] - 0s 225us/step - loss: 0.3426\n",
      "Epoch 216/800\n",
      "72/72 [==============================] - 0s 185us/step - loss: 0.3961\n",
      "Epoch 217/800\n",
      "72/72 [==============================] - 0s 253us/step - loss: 0.6267\n",
      "Epoch 218/800\n",
      "72/72 [==============================] - 0s 429us/step - loss: 0.3717\n",
      "Epoch 219/800\n",
      "72/72 [==============================] - 0s 449us/step - loss: 0.3713\n",
      "Epoch 220/800\n",
      "72/72 [==============================] - 0s 394us/step - loss: 0.3906\n",
      "Epoch 221/800\n",
      "72/72 [==============================] - 0s 366us/step - loss: 0.4726\n",
      "Epoch 222/800\n",
      "72/72 [==============================] - 0s 432us/step - loss: 0.3679\n",
      "Epoch 223/800\n",
      "72/72 [==============================] - 0s 388us/step - loss: 0.4194\n",
      "Epoch 224/800\n",
      "72/72 [==============================] - 0s 474us/step - loss: 0.3452\n",
      "Epoch 225/800\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.3229\n",
      "Epoch 226/800\n",
      "72/72 [==============================] - 0s 309us/step - loss: 0.5040\n",
      "Epoch 227/800\n",
      "72/72 [==============================] - 0s 404us/step - loss: 0.3745\n",
      "Epoch 228/800\n",
      "72/72 [==============================] - 0s 280us/step - loss: 0.5717\n",
      "Epoch 229/800\n",
      "72/72 [==============================] - 0s 414us/step - loss: 0.3953\n",
      "Epoch 230/800\n",
      "72/72 [==============================] - 0s 393us/step - loss: 0.3526\n",
      "Epoch 231/800\n",
      "72/72 [==============================] - 0s 444us/step - loss: 0.3928\n",
      "Epoch 232/800\n",
      "72/72 [==============================] - 0s 197us/step - loss: 0.3373\n",
      "Epoch 233/800\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.323 - 0s 258us/step - loss: 0.3329\n",
      "Epoch 234/800\n",
      "72/72 [==============================] - 0s 354us/step - loss: 0.4220\n",
      "Epoch 235/800\n",
      "72/72 [==============================] - 0s 463us/step - loss: 0.3499\n",
      "Epoch 236/800\n",
      "72/72 [==============================] - 0s 466us/step - loss: 0.5466\n",
      "Epoch 237/800\n",
      "72/72 [==============================] - 0s 402us/step - loss: 0.3504\n",
      "Epoch 238/800\n",
      "72/72 [==============================] - 0s 484us/step - loss: 0.3602\n",
      "Epoch 239/800\n",
      "72/72 [==============================] - 0s 482us/step - loss: 0.3680\n",
      "Epoch 240/800\n",
      "72/72 [==============================] - 0s 248us/step - loss: 0.3138\n",
      "Epoch 241/800\n",
      "72/72 [==============================] - 0s 276us/step - loss: 0.4552\n",
      "Epoch 242/800\n",
      "72/72 [==============================] - 0s 378us/step - loss: 0.4904\n",
      "Epoch 243/800\n",
      "72/72 [==============================] - 0s 211us/step - loss: 0.3513\n",
      "Epoch 244/800\n",
      "72/72 [==============================] - 0s 171us/step - loss: 0.3488\n",
      "Epoch 245/800\n",
      "72/72 [==============================] - 0s 202us/step - loss: 0.5837\n",
      "Epoch 246/800\n",
      "72/72 [==============================] - 0s 171us/step - loss: 0.3105\n",
      "Epoch 247/800\n",
      "72/72 [==============================] - 0s 160us/step - loss: 0.3123\n",
      "Epoch 248/800\n",
      "72/72 [==============================] - 0s 458us/step - loss: 0.2942\n",
      "Epoch 249/800\n",
      "72/72 [==============================] - 0s 295us/step - loss: 0.3417\n",
      "Epoch 250/800\n",
      "72/72 [==============================] - 0s 386us/step - loss: 0.3737\n",
      "Epoch 251/800\n",
      "72/72 [==============================] - 0s 428us/step - loss: 0.3596\n",
      "Epoch 252/800\n",
      "72/72 [==============================] - 0s 280us/step - loss: 0.5198\n",
      "Epoch 253/800\n",
      "72/72 [==============================] - 0s 396us/step - loss: 0.3277\n",
      "Epoch 254/800\n",
      "72/72 [==============================] - 0s 261us/step - loss: 0.3205\n",
      "Epoch 255/800\n",
      "72/72 [==============================] - 0s 321us/step - loss: 0.4196\n",
      "Epoch 256/800\n",
      "72/72 [==============================] - 0s 267us/step - loss: 0.3411\n",
      "Epoch 257/800\n",
      "72/72 [==============================] - 0s 353us/step - loss: 0.3477\n",
      "Epoch 258/800\n",
      "72/72 [==============================] - 0s 356us/step - loss: 0.3271\n",
      "Epoch 259/800\n",
      "72/72 [==============================] - 0s 337us/step - loss: 0.3081\n",
      "Epoch 260/800\n",
      "72/72 [==============================] - 0s 354us/step - loss: 0.3059\n",
      "Epoch 261/800\n",
      "72/72 [==============================] - 0s 341us/step - loss: 0.3945\n",
      "Epoch 262/800\n",
      "72/72 [==============================] - 0s 317us/step - loss: 0.3900\n",
      "Epoch 263/800\n",
      "72/72 [==============================] - 0s 392us/step - loss: 0.3440\n",
      "Epoch 264/800\n",
      "72/72 [==============================] - 0s 411us/step - loss: 0.3438\n",
      "Epoch 265/800\n",
      "72/72 [==============================] - 0s 285us/step - loss: 0.3437\n",
      "Epoch 266/800\n",
      "72/72 [==============================] - 0s 472us/step - loss: 0.2884\n",
      "Epoch 267/800\n",
      "72/72 [==============================] - 0s 213us/step - loss: 0.3760\n",
      "Epoch 268/800\n",
      "72/72 [==============================] - 0s 214us/step - loss: 0.4491\n",
      "Epoch 269/800\n",
      "72/72 [==============================] - 0s 327us/step - loss: 0.3280\n",
      "Epoch 270/800\n",
      "72/72 [==============================] - 0s 278us/step - loss: 0.3245\n",
      "Epoch 271/800\n",
      "72/72 [==============================] - 0s 401us/step - loss: 0.3579\n",
      "Epoch 272/800\n",
      "72/72 [==============================] - 0s 298us/step - loss: 0.3566\n",
      "Epoch 273/800\n",
      "72/72 [==============================] - 0s 250us/step - loss: 0.4578\n",
      "Epoch 274/800\n",
      "72/72 [==============================] - 0s 275us/step - loss: 0.2682\n",
      "Epoch 275/800\n",
      "72/72 [==============================] - 0s 197us/step - loss: 0.4359\n",
      "Epoch 276/800\n",
      "72/72 [==============================] - 0s 377us/step - loss: 0.3675\n",
      "Epoch 277/800\n",
      "72/72 [==============================] - 0s 242us/step - loss: 0.4488\n",
      "Epoch 278/800\n",
      "72/72 [==============================] - 0s 201us/step - loss: 0.3766\n",
      "Epoch 279/800\n",
      "72/72 [==============================] - 0s 279us/step - loss: 0.3790\n",
      "Epoch 280/800\n",
      "72/72 [==============================] - 0s 233us/step - loss: 0.3430\n",
      "Epoch 281/800\n",
      "72/72 [==============================] - 0s 284us/step - loss: 0.2961\n",
      "Epoch 282/800\n",
      "72/72 [==============================] - 0s 359us/step - loss: 0.3442\n",
      "Epoch 283/800\n",
      "72/72 [==============================] - 0s 280us/step - loss: 0.3580\n",
      "Epoch 284/800\n",
      "72/72 [==============================] - 0s 320us/step - loss: 0.2982\n",
      "Epoch 285/800\n",
      "72/72 [==============================] - 0s 327us/step - loss: 0.3213\n",
      "Epoch 286/800\n",
      "72/72 [==============================] - 0s 432us/step - loss: 0.2727\n",
      "Epoch 287/800\n",
      "72/72 [==============================] - 0s 339us/step - loss: 0.3851\n",
      "Epoch 288/800\n",
      "72/72 [==============================] - 0s 323us/step - loss: 0.3463\n",
      "Epoch 289/800\n",
      "72/72 [==============================] - 0s 282us/step - loss: 0.3695\n",
      "Epoch 290/800\n",
      "72/72 [==============================] - 0s 290us/step - loss: 0.3087\n",
      "Epoch 291/800\n",
      "72/72 [==============================] - 0s 407us/step - loss: 0.3259\n",
      "Epoch 292/800\n",
      "72/72 [==============================] - 0s 317us/step - loss: 0.3809\n",
      "Epoch 293/800\n",
      "72/72 [==============================] - 0s 262us/step - loss: 0.2968\n",
      "Epoch 294/800\n",
      "72/72 [==============================] - 0s 285us/step - loss: 0.3483\n",
      "Epoch 295/800\n",
      "72/72 [==============================] - 0s 302us/step - loss: 0.3091\n",
      "Epoch 296/800\n",
      "72/72 [==============================] - 0s 314us/step - loss: 0.2985\n",
      "Epoch 297/800\n",
      "72/72 [==============================] - 0s 411us/step - loss: 0.3604\n",
      "Epoch 298/800\n",
      "72/72 [==============================] - 0s 387us/step - loss: 0.2650\n",
      "Epoch 299/800\n",
      "72/72 [==============================] - 0s 255us/step - loss: 0.5662\n",
      "Epoch 300/800\n",
      "72/72 [==============================] - 0s 308us/step - loss: 0.2765\n",
      "Epoch 301/800\n",
      "72/72 [==============================] - 0s 379us/step - loss: 0.3478\n",
      "Epoch 302/800\n",
      "72/72 [==============================] - 0s 395us/step - loss: 0.3146\n",
      "Epoch 303/800\n",
      "72/72 [==============================] - 0s 340us/step - loss: 0.3105\n",
      "Epoch 304/800\n",
      "72/72 [==============================] - 0s 427us/step - loss: 0.2751\n",
      "Epoch 305/800\n",
      "72/72 [==============================] - 0s 288us/step - loss: 0.3287\n",
      "Epoch 306/800\n",
      "72/72 [==============================] - 0s 453us/step - loss: 0.3352\n",
      "Epoch 307/800\n",
      "72/72 [==============================] - 0s 408us/step - loss: 0.4303\n",
      "Epoch 308/800\n",
      "72/72 [==============================] - 0s 262us/step - loss: 0.2790\n",
      "Epoch 309/800\n",
      "72/72 [==============================] - 0s 297us/step - loss: 0.3700\n",
      "Epoch 310/800\n",
      "72/72 [==============================] - 0s 476us/step - loss: 0.3431\n",
      "Epoch 311/800\n",
      "72/72 [==============================] - 0s 405us/step - loss: 0.2699\n",
      "Epoch 312/800\n",
      "72/72 [==============================] - 0s 307us/step - loss: 0.2908\n",
      "Epoch 313/800\n",
      "72/72 [==============================] - 0s 221us/step - loss: 0.2588\n",
      "Epoch 314/800\n",
      "72/72 [==============================] - 0s 353us/step - loss: 0.2886\n",
      "Epoch 315/800\n",
      "72/72 [==============================] - 0s 326us/step - loss: 0.3308\n",
      "Epoch 316/800\n",
      "72/72 [==============================] - 0s 321us/step - loss: 0.3010\n",
      "Epoch 317/800\n",
      "72/72 [==============================] - 0s 244us/step - loss: 0.3533\n",
      "Epoch 318/800\n",
      "72/72 [==============================] - 0s 265us/step - loss: 0.4068\n",
      "Epoch 319/800\n",
      "72/72 [==============================] - 0s 329us/step - loss: 0.2735\n",
      "Epoch 320/800\n",
      "72/72 [==============================] - 0s 341us/step - loss: 0.3143\n",
      "Epoch 321/800\n",
      "72/72 [==============================] - 0s 338us/step - loss: 0.3633\n",
      "Epoch 322/800\n",
      "72/72 [==============================] - 0s 442us/step - loss: 0.3334\n",
      "Epoch 323/800\n",
      "72/72 [==============================] - 0s 455us/step - loss: 0.3512\n",
      "Epoch 324/800\n",
      "72/72 [==============================] - 0s 238us/step - loss: 0.3440\n",
      "Epoch 325/800\n",
      "72/72 [==============================] - 0s 289us/step - loss: 0.2886\n",
      "Epoch 326/800\n",
      "72/72 [==============================] - 0s 345us/step - loss: 0.4590\n",
      "Epoch 327/800\n",
      "72/72 [==============================] - 0s 285us/step - loss: 0.3299\n",
      "Epoch 328/800\n",
      "72/72 [==============================] - 0s 234us/step - loss: 0.2948\n",
      "Epoch 329/800\n",
      "72/72 [==============================] - 0s 162us/step - loss: 0.2637\n",
      "Epoch 330/800\n",
      "72/72 [==============================] - 0s 360us/step - loss: 0.2815\n",
      "Epoch 331/800\n",
      "72/72 [==============================] - 0s 411us/step - loss: 0.3660\n",
      "Epoch 332/800\n",
      "72/72 [==============================] - 0s 352us/step - loss: 0.2811\n",
      "Epoch 333/800\n",
      "72/72 [==============================] - 0s 262us/step - loss: 0.2781\n",
      "Epoch 334/800\n",
      "72/72 [==============================] - 0s 288us/step - loss: 0.3049\n",
      "Epoch 335/800\n",
      "72/72 [==============================] - 0s 289us/step - loss: 0.2715\n",
      "Epoch 336/800\n",
      "72/72 [==============================] - 0s 222us/step - loss: 0.3448\n",
      "Epoch 337/800\n",
      "72/72 [==============================] - 0s 196us/step - loss: 0.2218\n",
      "Epoch 338/800\n",
      "72/72 [==============================] - 0s 220us/step - loss: 0.2756\n",
      "Epoch 339/800\n",
      "72/72 [==============================] - 0s 281us/step - loss: 0.3133\n",
      "Epoch 340/800\n",
      "72/72 [==============================] - 0s 292us/step - loss: 0.2625\n",
      "Epoch 341/800\n",
      "72/72 [==============================] - 0s 349us/step - loss: 0.3517\n",
      "Epoch 342/800\n",
      "72/72 [==============================] - 0s 381us/step - loss: 0.3689\n",
      "Epoch 343/800\n",
      "72/72 [==============================] - 0s 263us/step - loss: 0.2729\n",
      "Epoch 344/800\n",
      "72/72 [==============================] - 0s 267us/step - loss: 0.2283\n",
      "Epoch 345/800\n",
      "72/72 [==============================] - 0s 414us/step - loss: 0.4356\n",
      "Epoch 346/800\n",
      "72/72 [==============================] - 0s 457us/step - loss: 0.2969\n",
      "Epoch 347/800\n",
      "72/72 [==============================] - 0s 352us/step - loss: 0.2672\n",
      "Epoch 348/800\n",
      "72/72 [==============================] - 0s 326us/step - loss: 0.2839\n",
      "Epoch 349/800\n",
      "72/72 [==============================] - 0s 242us/step - loss: 0.2962\n",
      "Epoch 350/800\n",
      "72/72 [==============================] - 0s 321us/step - loss: 0.2339\n",
      "Epoch 351/800\n",
      "72/72 [==============================] - 0s 307us/step - loss: 0.3090\n",
      "Epoch 352/800\n",
      "72/72 [==============================] - 0s 327us/step - loss: 0.2700\n",
      "Epoch 353/800\n",
      "72/72 [==============================] - 0s 224us/step - loss: 0.3292\n",
      "Epoch 354/800\n",
      "72/72 [==============================] - 0s 256us/step - loss: 0.2711\n",
      "Epoch 355/800\n",
      "72/72 [==============================] - 0s 278us/step - loss: 0.2924\n",
      "Epoch 356/800\n",
      "72/72 [==============================] - 0s 396us/step - loss: 0.2585\n",
      "Epoch 357/800\n",
      "72/72 [==============================] - 0s 370us/step - loss: 0.2698\n",
      "Epoch 358/800\n",
      "72/72 [==============================] - 0s 300us/step - loss: 0.3435\n",
      "Epoch 359/800\n",
      "72/72 [==============================] - 0s 293us/step - loss: 0.2723\n",
      "Epoch 360/800\n",
      "72/72 [==============================] - 0s 276us/step - loss: 0.4473\n",
      "Epoch 361/800\n",
      "72/72 [==============================] - 0s 252us/step - loss: 0.2393\n",
      "Epoch 362/800\n",
      "72/72 [==============================] - 0s 233us/step - loss: 0.2858\n",
      "Epoch 363/800\n",
      "72/72 [==============================] - 0s 268us/step - loss: 0.2106\n",
      "Epoch 364/800\n",
      "72/72 [==============================] - 0s 281us/step - loss: 0.2868\n",
      "Epoch 365/800\n",
      "72/72 [==============================] - 0s 236us/step - loss: 0.3629\n",
      "Epoch 366/800\n",
      "72/72 [==============================] - 0s 385us/step - loss: 0.3198\n",
      "Epoch 367/800\n",
      "72/72 [==============================] - 0s 242us/step - loss: 0.2561\n",
      "Epoch 368/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 292us/step - loss: 0.1967\n",
      "Epoch 369/800\n",
      "72/72 [==============================] - 0s 291us/step - loss: 0.2267\n",
      "Epoch 370/800\n",
      "72/72 [==============================] - 0s 279us/step - loss: 0.2821\n",
      "Epoch 371/800\n",
      "72/72 [==============================] - 0s 366us/step - loss: 0.2875\n",
      "Epoch 372/800\n",
      "72/72 [==============================] - 0s 324us/step - loss: 0.2554\n",
      "Epoch 373/800\n",
      "72/72 [==============================] - 0s 382us/step - loss: 0.2435\n",
      "Epoch 374/800\n",
      "72/72 [==============================] - 0s 329us/step - loss: 0.2414\n",
      "Epoch 375/800\n",
      "72/72 [==============================] - 0s 319us/step - loss: 0.2966\n",
      "Epoch 376/800\n",
      "72/72 [==============================] - 0s 351us/step - loss: 0.2374\n",
      "Epoch 377/800\n",
      "72/72 [==============================] - 0s 291us/step - loss: 0.3252\n",
      "Epoch 378/800\n",
      "72/72 [==============================] - 0s 404us/step - loss: 0.2582\n",
      "Epoch 379/800\n",
      "72/72 [==============================] - 0s 269us/step - loss: 0.3155\n",
      "Epoch 380/800\n",
      "72/72 [==============================] - 0s 299us/step - loss: 0.2159\n",
      "Epoch 381/800\n",
      "72/72 [==============================] - 0s 403us/step - loss: 0.4021\n",
      "Epoch 382/800\n",
      "72/72 [==============================] - 0s 436us/step - loss: 0.2178\n",
      "Epoch 383/800\n",
      "72/72 [==============================] - 0s 431us/step - loss: 0.2416\n",
      "Epoch 384/800\n",
      "72/72 [==============================] - 0s 395us/step - loss: 0.2160\n",
      "Epoch 385/800\n",
      "72/72 [==============================] - 0s 453us/step - loss: 0.2169\n",
      "Epoch 386/800\n",
      "72/72 [==============================] - 0s 310us/step - loss: 0.2838\n",
      "Epoch 387/800\n",
      "72/72 [==============================] - 0s 281us/step - loss: 0.2580\n",
      "Epoch 388/800\n",
      "72/72 [==============================] - 0s 256us/step - loss: 0.3192\n",
      "Epoch 389/800\n",
      "72/72 [==============================] - 0s 377us/step - loss: 0.2676\n",
      "Epoch 390/800\n",
      "72/72 [==============================] - 0s 300us/step - loss: 0.2503\n",
      "Epoch 391/800\n",
      "72/72 [==============================] - 0s 319us/step - loss: 0.2361\n",
      "Epoch 392/800\n",
      "72/72 [==============================] - 0s 285us/step - loss: 0.2716\n",
      "Epoch 393/800\n",
      "72/72 [==============================] - 0s 360us/step - loss: 0.2070\n",
      "Epoch 394/800\n",
      "72/72 [==============================] - 0s 331us/step - loss: 0.2754\n",
      "Epoch 395/800\n",
      "72/72 [==============================] - 0s 364us/step - loss: 0.1727\n",
      "Epoch 396/800\n",
      "72/72 [==============================] - 0s 320us/step - loss: 0.2487\n",
      "Epoch 397/800\n",
      "72/72 [==============================] - 0s 292us/step - loss: 0.2464\n",
      "Epoch 398/800\n",
      "72/72 [==============================] - 0s 308us/step - loss: 0.2513\n",
      "Epoch 399/800\n",
      "72/72 [==============================] - 0s 476us/step - loss: 0.2069\n",
      "Epoch 400/800\n",
      "72/72 [==============================] - 0s 421us/step - loss: 0.1928\n",
      "Epoch 401/800\n",
      "72/72 [==============================] - 0s 309us/step - loss: 0.2836\n",
      "Epoch 402/800\n",
      "72/72 [==============================] - 0s 358us/step - loss: 0.2251\n",
      "Epoch 403/800\n",
      "72/72 [==============================] - 0s 352us/step - loss: 0.2399\n",
      "Epoch 404/800\n",
      "72/72 [==============================] - 0s 303us/step - loss: 0.2169\n",
      "Epoch 405/800\n",
      "72/72 [==============================] - 0s 442us/step - loss: 0.1830\n",
      "Epoch 406/800\n",
      "72/72 [==============================] - 0s 387us/step - loss: 0.2447\n",
      "Epoch 407/800\n",
      "72/72 [==============================] - 0s 410us/step - loss: 0.2322\n",
      "Epoch 408/800\n",
      "72/72 [==============================] - 0s 339us/step - loss: 0.2909\n",
      "Epoch 409/800\n",
      "72/72 [==============================] - 0s 364us/step - loss: 0.2199\n",
      "Epoch 410/800\n",
      "72/72 [==============================] - 0s 388us/step - loss: 0.3257\n",
      "Epoch 411/800\n",
      "72/72 [==============================] - 0s 296us/step - loss: 0.2333\n",
      "Epoch 412/800\n",
      "72/72 [==============================] - 0s 418us/step - loss: 0.2598\n",
      "Epoch 413/800\n",
      "72/72 [==============================] - 0s 399us/step - loss: 0.1839\n",
      "Epoch 414/800\n",
      "72/72 [==============================] - 0s 383us/step - loss: 0.2831\n",
      "Epoch 415/800\n",
      "72/72 [==============================] - 0s 294us/step - loss: 0.2279\n",
      "Epoch 416/800\n",
      "72/72 [==============================] - 0s 409us/step - loss: 0.1689\n",
      "Epoch 417/800\n",
      "72/72 [==============================] - 0s 268us/step - loss: 0.2603\n",
      "Epoch 418/800\n",
      "72/72 [==============================] - 0s 313us/step - loss: 0.1608\n",
      "Epoch 419/800\n",
      "72/72 [==============================] - 0s 332us/step - loss: 0.1900\n",
      "Epoch 420/800\n",
      "72/72 [==============================] - 0s 377us/step - loss: 0.2309\n",
      "Epoch 421/800\n",
      "72/72 [==============================] - 0s 309us/step - loss: 0.2589\n",
      "Epoch 422/800\n",
      "72/72 [==============================] - 0s 216us/step - loss: 0.1660\n",
      "Epoch 423/800\n",
      "72/72 [==============================] - 0s 272us/step - loss: 0.2156\n",
      "Epoch 424/800\n",
      "72/72 [==============================] - 0s 234us/step - loss: 0.1570\n",
      "Epoch 425/800\n",
      "72/72 [==============================] - 0s 301us/step - loss: 0.2429\n",
      "Epoch 426/800\n",
      "72/72 [==============================] - 0s 445us/step - loss: 0.2552\n",
      "Epoch 427/800\n",
      "72/72 [==============================] - 0s 440us/step - loss: 0.1297\n",
      "Epoch 428/800\n",
      "72/72 [==============================] - 0s 320us/step - loss: 0.2180\n",
      "Epoch 429/800\n",
      "72/72 [==============================] - 0s 201us/step - loss: 0.2746\n",
      "Epoch 430/800\n",
      "72/72 [==============================] - 0s 223us/step - loss: 0.2539\n",
      "Epoch 431/800\n",
      "72/72 [==============================] - 0s 201us/step - loss: 0.1727\n",
      "Epoch 432/800\n",
      "72/72 [==============================] - 0s 255us/step - loss: 0.2308\n",
      "Epoch 433/800\n",
      "72/72 [==============================] - 0s 200us/step - loss: 0.2494\n",
      "Epoch 434/800\n",
      "72/72 [==============================] - 0s 184us/step - loss: 0.1731\n",
      "Epoch 435/800\n",
      "72/72 [==============================] - 0s 262us/step - loss: 0.2401\n",
      "Epoch 436/800\n",
      "72/72 [==============================] - 0s 364us/step - loss: 0.2281\n",
      "Epoch 437/800\n",
      "72/72 [==============================] - 0s 400us/step - loss: 0.2128\n",
      "Epoch 438/800\n",
      "72/72 [==============================] - 0s 353us/step - loss: 0.2927\n",
      "Epoch 439/800\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.2435\n",
      "Epoch 440/800\n",
      "72/72 [==============================] - 0s 275us/step - loss: 0.2329\n",
      "Epoch 441/800\n",
      "72/72 [==============================] - 0s 427us/step - loss: 0.1430\n",
      "Epoch 442/800\n",
      "72/72 [==============================] - 0s 361us/step - loss: 0.1396\n",
      "Epoch 443/800\n",
      "72/72 [==============================] - 0s 339us/step - loss: 0.2413\n",
      "Epoch 444/800\n",
      "72/72 [==============================] - 0s 429us/step - loss: 0.2729\n",
      "Epoch 445/800\n",
      "72/72 [==============================] - 0s 305us/step - loss: 0.2021\n",
      "Epoch 446/800\n",
      "72/72 [==============================] - 0s 342us/step - loss: 0.1991\n",
      "Epoch 447/800\n",
      "72/72 [==============================] - 0s 363us/step - loss: 0.1835\n",
      "Epoch 448/800\n",
      "72/72 [==============================] - 0s 276us/step - loss: 0.2021\n",
      "Epoch 449/800\n",
      "72/72 [==============================] - 0s 372us/step - loss: 0.2441\n",
      "Epoch 450/800\n",
      "72/72 [==============================] - 0s 365us/step - loss: 0.1358\n",
      "Epoch 451/800\n",
      "72/72 [==============================] - 0s 216us/step - loss: 0.3120\n",
      "Epoch 452/800\n",
      "72/72 [==============================] - 0s 262us/step - loss: 0.1770\n",
      "Epoch 453/800\n",
      "72/72 [==============================] - 0s 403us/step - loss: 0.1828\n",
      "Epoch 454/800\n",
      "72/72 [==============================] - 0s 350us/step - loss: 0.1884\n",
      "Epoch 455/800\n",
      "72/72 [==============================] - 0s 229us/step - loss: 0.1889\n",
      "Epoch 456/800\n",
      "72/72 [==============================] - 0s 264us/step - loss: 0.2114\n",
      "Epoch 457/800\n",
      "72/72 [==============================] - 0s 211us/step - loss: 0.1689\n",
      "Epoch 458/800\n",
      "72/72 [==============================] - 0s 316us/step - loss: 0.2111\n",
      "Epoch 459/800\n",
      "72/72 [==============================] - 0s 318us/step - loss: 0.1909\n",
      "Epoch 460/800\n",
      "72/72 [==============================] - 0s 359us/step - loss: 0.1887\n",
      "Epoch 461/800\n",
      "72/72 [==============================] - 0s 327us/step - loss: 0.2218\n",
      "Epoch 462/800\n",
      "72/72 [==============================] - 0s 344us/step - loss: 0.1178\n",
      "Epoch 463/800\n",
      "72/72 [==============================] - 0s 332us/step - loss: 0.1620\n",
      "Epoch 464/800\n",
      "72/72 [==============================] - 0s 261us/step - loss: 0.1981\n",
      "Epoch 465/800\n",
      "72/72 [==============================] - 0s 281us/step - loss: 0.4212\n",
      "Epoch 466/800\n",
      "72/72 [==============================] - 0s 326us/step - loss: 0.1582\n",
      "Epoch 467/800\n",
      "72/72 [==============================] - 0s 273us/step - loss: 0.2116\n",
      "Epoch 468/800\n",
      "72/72 [==============================] - 0s 255us/step - loss: 0.1480\n",
      "Epoch 469/800\n",
      "72/72 [==============================] - 0s 243us/step - loss: 0.2553\n",
      "Epoch 470/800\n",
      "72/72 [==============================] - 0s 335us/step - loss: 0.1081\n",
      "Epoch 471/800\n",
      "72/72 [==============================] - 0s 233us/step - loss: 0.2608\n",
      "Epoch 472/800\n",
      "72/72 [==============================] - 0s 252us/step - loss: 0.1634\n",
      "Epoch 473/800\n",
      "72/72 [==============================] - 0s 275us/step - loss: 0.1863\n",
      "Epoch 474/800\n",
      "72/72 [==============================] - 0s 341us/step - loss: 0.2134\n",
      "Epoch 475/800\n",
      "72/72 [==============================] - 0s 279us/step - loss: 0.2271\n",
      "Epoch 476/800\n",
      "72/72 [==============================] - 0s 295us/step - loss: 0.1462\n",
      "Epoch 477/800\n",
      "72/72 [==============================] - 0s 384us/step - loss: 0.2764\n",
      "Epoch 478/800\n",
      "72/72 [==============================] - 0s 413us/step - loss: 0.1849\n",
      "Epoch 479/800\n",
      "72/72 [==============================] - 0s 408us/step - loss: 0.1994\n",
      "Epoch 480/800\n",
      "72/72 [==============================] - 0s 435us/step - loss: 0.2370\n",
      "Epoch 481/800\n",
      "72/72 [==============================] - 0s 462us/step - loss: 0.1462\n",
      "Epoch 482/800\n",
      "72/72 [==============================] - 0s 372us/step - loss: 0.1759\n",
      "Epoch 483/800\n",
      "72/72 [==============================] - 0s 296us/step - loss: 0.2418\n",
      "Epoch 484/800\n",
      "72/72 [==============================] - 0s 401us/step - loss: 0.2673\n",
      "Epoch 485/800\n",
      "72/72 [==============================] - 0s 294us/step - loss: 0.1382\n",
      "Epoch 486/800\n",
      "72/72 [==============================] - 0s 344us/step - loss: 0.2388\n",
      "Epoch 487/800\n",
      "72/72 [==============================] - 0s 342us/step - loss: 0.2303\n",
      "Epoch 488/800\n",
      "72/72 [==============================] - 0s 353us/step - loss: 0.2016\n",
      "Epoch 489/800\n",
      "72/72 [==============================] - 0s 290us/step - loss: 0.1456\n",
      "Epoch 490/800\n",
      "72/72 [==============================] - 0s 282us/step - loss: 0.2251\n",
      "Epoch 491/800\n",
      "72/72 [==============================] - 0s 366us/step - loss: 0.1770\n",
      "Epoch 492/800\n",
      "72/72 [==============================] - 0s 356us/step - loss: 0.1572\n",
      "Epoch 493/800\n",
      "72/72 [==============================] - 0s 293us/step - loss: 0.2142\n",
      "Epoch 494/800\n",
      "72/72 [==============================] - 0s 324us/step - loss: 0.2260\n",
      "Epoch 495/800\n",
      "72/72 [==============================] - 0s 326us/step - loss: 0.1502\n",
      "Epoch 496/800\n",
      "72/72 [==============================] - 0s 314us/step - loss: 0.2222\n",
      "Epoch 497/800\n",
      "72/72 [==============================] - 0s 451us/step - loss: 0.1663\n",
      "Epoch 498/800\n",
      "72/72 [==============================] - 0s 448us/step - loss: 0.1820\n",
      "Epoch 499/800\n",
      "72/72 [==============================] - 0s 250us/step - loss: 0.1942\n",
      "Epoch 500/800\n",
      "72/72 [==============================] - 0s 252us/step - loss: 0.1810\n",
      "Epoch 501/800\n",
      "72/72 [==============================] - 0s 286us/step - loss: 0.1995\n",
      "Epoch 502/800\n",
      "72/72 [==============================] - 0s 422us/step - loss: 0.2012\n",
      "Epoch 503/800\n",
      "72/72 [==============================] - 0s 276us/step - loss: 0.1938\n",
      "Epoch 504/800\n",
      "72/72 [==============================] - 0s 323us/step - loss: 0.1610\n",
      "Epoch 505/800\n",
      "72/72 [==============================] - 0s 240us/step - loss: 0.2177\n",
      "Epoch 506/800\n",
      "72/72 [==============================] - 0s 317us/step - loss: 0.2159\n",
      "Epoch 507/800\n",
      "72/72 [==============================] - 0s 400us/step - loss: 0.2343\n",
      "Epoch 508/800\n",
      "72/72 [==============================] - 0s 270us/step - loss: 0.1535\n",
      "Epoch 509/800\n",
      "72/72 [==============================] - 0s 427us/step - loss: 0.1930\n",
      "Epoch 510/800\n",
      "72/72 [==============================] - 0s 443us/step - loss: 0.1777\n",
      "Epoch 511/800\n",
      "72/72 [==============================] - 0s 431us/step - loss: 0.2510\n",
      "Epoch 512/800\n",
      "72/72 [==============================] - 0s 456us/step - loss: 0.1729\n",
      "Epoch 513/800\n",
      "72/72 [==============================] - 0s 406us/step - loss: 0.1558\n",
      "Epoch 514/800\n",
      "72/72 [==============================] - 0s 396us/step - loss: 0.1118\n",
      "Epoch 515/800\n",
      "72/72 [==============================] - 0s 413us/step - loss: 0.2098\n",
      "Epoch 516/800\n",
      "72/72 [==============================] - 0s 434us/step - loss: 0.1704\n",
      "Epoch 517/800\n",
      "72/72 [==============================] - 0s 325us/step - loss: 0.1966\n",
      "Epoch 518/800\n",
      "72/72 [==============================] - 0s 298us/step - loss: 0.2317\n",
      "Epoch 519/800\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2014\n",
      "Epoch 520/800\n",
      "72/72 [==============================] - 0s 353us/step - loss: 0.2270\n",
      "Epoch 521/800\n",
      "72/72 [==============================] - 0s 225us/step - loss: 0.1862\n",
      "Epoch 522/800\n",
      "72/72 [==============================] - 0s 267us/step - loss: 0.1322\n",
      "Epoch 523/800\n",
      "72/72 [==============================] - 0s 456us/step - loss: 0.2235\n",
      "Epoch 524/800\n",
      "72/72 [==============================] - 0s 311us/step - loss: 0.1878\n",
      "Epoch 525/800\n",
      "72/72 [==============================] - 0s 239us/step - loss: 0.2423\n",
      "Epoch 526/800\n",
      "72/72 [==============================] - 0s 245us/step - loss: 0.1529\n",
      "Epoch 527/800\n",
      "72/72 [==============================] - 0s 179us/step - loss: 0.2533\n",
      "Epoch 528/800\n",
      "72/72 [==============================] - 0s 269us/step - loss: 0.1972\n",
      "Epoch 529/800\n",
      "72/72 [==============================] - 0s 399us/step - loss: 0.2182\n",
      "Epoch 530/800\n",
      "72/72 [==============================] - 0s 247us/step - loss: 0.1350\n",
      "Epoch 531/800\n",
      "72/72 [==============================] - 0s 265us/step - loss: 0.1815\n",
      "Epoch 532/800\n",
      "72/72 [==============================] - 0s 279us/step - loss: 0.1454\n",
      "Epoch 533/800\n",
      "72/72 [==============================] - 0s 232us/step - loss: 0.2373\n",
      "Epoch 534/800\n",
      "72/72 [==============================] - 0s 305us/step - loss: 0.2021\n",
      "Epoch 535/800\n",
      "72/72 [==============================] - 0s 181us/step - loss: 0.1599\n",
      "Epoch 536/800\n",
      "72/72 [==============================] - 0s 243us/step - loss: 0.1954\n",
      "Epoch 537/800\n",
      "72/72 [==============================] - 0s 169us/step - loss: 0.1475\n",
      "Epoch 538/800\n",
      "72/72 [==============================] - 0s 166us/step - loss: 0.1595\n",
      "Epoch 539/800\n",
      "72/72 [==============================] - 0s 216us/step - loss: 0.1848\n",
      "Epoch 540/800\n",
      "72/72 [==============================] - 0s 219us/step - loss: 0.2002\n",
      "Epoch 541/800\n",
      "72/72 [==============================] - 0s 176us/step - loss: 0.1587\n",
      "Epoch 542/800\n",
      "72/72 [==============================] - 0s 398us/step - loss: 0.1528\n",
      "Epoch 543/800\n",
      "72/72 [==============================] - 0s 424us/step - loss: 0.1760\n",
      "Epoch 544/800\n",
      "72/72 [==============================] - 0s 382us/step - loss: 0.2499\n",
      "Epoch 545/800\n",
      "72/72 [==============================] - 0s 459us/step - loss: 0.1438\n",
      "Epoch 546/800\n",
      "72/72 [==============================] - 0s 349us/step - loss: 0.2119\n",
      "Epoch 547/800\n",
      "72/72 [==============================] - 0s 393us/step - loss: 0.2698\n",
      "Epoch 548/800\n",
      "72/72 [==============================] - 0s 390us/step - loss: 0.2440\n",
      "Epoch 549/800\n",
      "72/72 [==============================] - 0s 412us/step - loss: 0.1551\n",
      "Epoch 550/800\n",
      "72/72 [==============================] - 0s 388us/step - loss: 0.1481\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00550: early stopping\n",
      "best epoch =  470\n",
      "smallest loss = 0.10808792379167345\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(X_train,Y_train,epochs=700,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n",
    "\n",
    "model.save('./best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  50.8 , power=  102.5 ,  predicted voltage_to_load =  39.89040151834488 , predicted power=  49.107829958200455\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  200.0 , qsol=  24.3 , voltage_to_load=  27.899999999999995 , power=  32.1 ,  predicted voltage_to_load =  28.54030816257 , predicted power=  26.605170845985413\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  200.0 , qsol=  51.8 , voltage_to_load=  46.0 , power=  40.9 ,  predicted voltage_to_load =  41.23352357149124 , predicted power=  28.476168677210808\n",
      "row [0] data:  M=  2.0 , T1=  20.0 , gam=  500.0 , qsol=  79.1 , voltage_to_load=  185.7573 , power=  436.22976616042985 ,  predicted voltage_to_load =  149.33984608650206 , predicted power=  224.5931958913803\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  101.6 , power=  248.3 ,  predicted voltage_to_load =  68.2640061020851 , predicted power=  125.26110007166864\n",
      "row [0] data:  M=  2.0 , T1=  20.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  194.86305 , power=  912.7790445986176 ,  predicted voltage_to_load =  153.7622031211853 , predicted power=  686.2261065959931\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  200.0 , qsol=  51.8 , voltage_to_load=  92.1 , power=  163.9 ,  predicted voltage_to_load =  68.2298061132431 , predicted power=  74.87419908344746\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  184.2 , power=  199.6 ,  predicted voltage_to_load =  139.3638368844986 , predicted power=  99.3396065711975\n",
      "row [0] data:  M=  0.0 , T1=  20.0 , gam=  500.0 , qsol=  21.2 , voltage_to_load=  46.439325 , power=  101.72692954979364 ,  predicted voltage_to_load =  41.184130162000656 , predicted power=  61.9113135099411\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  148.6 , power=  877.2 ,  predicted voltage_to_load =  146.01146793365479 , predicted power=  811.7359965801239\n",
      "row [0] data:  M=  0.0 , T1=  20.0 , gam=  200.0 , qsol=  24.3 , voltage_to_load=  44.162887500000004 , power=  80.26175441718752 ,  predicted voltage_to_load =  42.28353291749954 , predicted power=  64.47297716736794\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  1000.0 , qsol=  3.92 , voltage_to_load=  23.1 , power=  136.4 ,  predicted voltage_to_load =  27.047003978490828 , predicted power=  163.52298706769943\n",
      "row [0] data:  M=  0.0 , T1=  20.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  44.162887500000004 , power=  11.465964916775999 ,  predicted voltage_to_load =  34.78628000020981 , predicted power=  -11.109828987717629\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  98.6 , power=  327.4 ,  predicted voltage_to_load =  76.4324255824089 , predicted power=  200.17617057561876\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  49.3 , power=  81.8 ,  predicted voltage_to_load =  40.029430192708965 , predicted power=  40.67190310955048\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  200.0 , qsol=  96.2 , voltage_to_load=  110.63 , power=  127.22449999999999 ,  predicted voltage_to_load =  108.20804808735848 , predicted power=  94.54402656555176\n",
      "row [0] data:  M=  0.0 , T1=  30.0 , gam=  200.0 , qsol=  96.2 , voltage_to_load=  42.250775 , power=  18.556423992729986 ,  predicted voltage_to_load =  37.31370322406292 , predicted power=  5.23870439529419\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  203.2 , power=  993.5 ,  predicted voltage_to_load =  162.1303263425827 , predicted power=  774.6643597602845\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  20.4 , power=  85.6 ,  predicted voltage_to_load =  22.895738723874093 , predicted power=  106.52190443873405\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  148.68 , power=  877.2120000000001 ,  predicted voltage_to_load =  140.619624710083 , predicted power=  780.9285851478577\n",
      "row [0] data:  M=  0.0 , T1=  30.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  45.212685 , power=  68.82784124273485 ,  predicted voltage_to_load =  38.55940921902656 , predicted power=  32.999580934643745\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  20.86 , power=  62.16279999999999 ,  predicted voltage_to_load =  23.917729653418064 , predicted power=  78.34030146598816\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  1000.0 , qsol=  11.7 , voltage_to_load=  93.21305 , power=  742.6215974617521 ,  predicted voltage_to_load =  67.89277646541595 , predicted power=  639.6519664764404\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  20.86 , power=  124.32559999999998 ,  predicted voltage_to_load =  22.361099278926847 , predicted power=  135.52837693691254\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  200.0 , qsol=  51.8 , voltage_to_load=  84.50155 , power=  137.84772108885133 ,  predicted voltage_to_load =  60.55294079780578 , predicted power=  61.76294881105423\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  500.0 , qsol=  21.2 , voltage_to_load=  48.4 , power=  110.7 ,  predicted voltage_to_load =  41.94135552048683 , predicted power=  66.89936771392823\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  200.0 , qsol=  24.3 , voltage_to_load=  27.899999999999995 , power=  64.2 ,  predicted voltage_to_load =  37.55653826594352 , predicted power=  68.88488709628582\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  700.0 , qsol=  55.3 , voltage_to_load=  197.2 , power=  703.3 ,  predicted voltage_to_load =  165.6745549440384 , predicted power=  575.7313528537751\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  200.0 , qsol=  96.2 , voltage_to_load=  110.60000000000001 , power=  127.2 ,  predicted voltage_to_load =  115.52196092605591 , predicted power=  120.56688039302826\n",
      "row [0] data:  M=  2.0 , T1=  20.0 , gam=  200.0 , qsol=  51.8 , voltage_to_load=  59.569999999999986 , power=  68.505499999998 ,  predicted voltage_to_load =  61.0312015414238 , predicted power=  63.47196311056614\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  500.0 , qsol=  79.1 , voltage_to_load=  88.8573 , power=  99.81820181150441 ,  predicted voltage_to_load =  55.83200116157531 , predicted power=  37.393032321333884\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  94.517685 , power=  300.7943696215227 ,  predicted voltage_to_load =  70.4405244231224 , predicted power=  162.51471593379975\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  500.0 , qsol=  43.2 , voltage_to_load=  96.9 , power=  217.29999999999998 ,  predicted voltage_to_load =  78.07846933603287 , predicted power=  117.94354898929596\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  1000.0 , qsol=  3.92 , voltage_to_load=  23.128 , power=  272.91040000000004 ,  predicted voltage_to_load =  29.521979987621307 , predicted power=  272.3010118246079\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  20.4 , power=  171.2 ,  predicted voltage_to_load =  23.809079602360725 , predicted power=  194.78038702011108\n",
      "row [0] data:  M=  0.0 , T1=  20.0 , gam=  700.0 , qsol=  14.3 , voltage_to_load=  47.2588425 , power=  156.18169191886756 ,  predicted voltage_to_load =  40.881826454401015 , predicted power=  97.85185105204583\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  46.0 , power=  12.4 ,  predicted voltage_to_load =  35.21790200471878 , predicted power=  -10.720602944493294\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  500.0 , qsol=  79.1 , voltage_to_load=  48.4 , power=  29.599999999999998 ,  predicted voltage_to_load =  37.480761820077895 , predicted power=  8.428629979491234\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  500.0 , qsol=  43.2 , voltage_to_load=  128.73600000000002 , power=  383.6332807 ,  predicted voltage_to_load =  120.70281875133514 , predicted power=  287.6980523705483\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  1000.0 , qsol=  3.92 , voltage_to_load=  50.8 , power=  658.9 ,  predicted voltage_to_load =  39.83772091269493 , predicted power=  563.8725228786469\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  500.0 , qsol=  43.2 , voltage_to_load=  92.87865 , power=  199.68619504218745 ,  predicted voltage_to_load =  70.82869626879692 , predicted power=  97.72740138173104\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  1000.0 , qsol=  3.92 , voltage_to_load=  23.128 , power=  136.45520000000002 ,  predicted voltage_to_load =  24.735559403896332 , predicted power=  141.2995985865593\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  1000.0 , qsol=  11.7 , voltage_to_load=  69.0 , power=  407.2 ,  predicted voltage_to_load =  67.04902552962302 , predicted power=  373.4225068807602\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  49.3 , power=  496.1000000000001 ,  predicted voltage_to_load =  38.08389328420162 , predicted power=  417.40896928310394\n",
      "row [0] data:  M=  2.0 , T1=  20.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  176.65155000000001 , power=  183.45543866772 ,  predicted voltage_to_load =  130.45172214508057 , predicted power=  89.07158360481263\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  20.8 , power=  124.3 ,  predicted voltage_to_load =  22.52410640567541 , predicted power=  139.36363524198532\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  200.0 , qsol=  24.3 , voltage_to_load=  27.945 , power=  64.2735 ,  predicted voltage_to_load =  38.52295750081539 , predicted power=  71.99190094769001\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  101.6 , power=  410.0 ,  predicted voltage_to_load =  70.89058102369309 , predicted power=  266.29266928434373\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  20.482 , power=  85.61475999999999 ,  predicted voltage_to_load =  22.171454358100892 , predicted power=  104.236656332016\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  500.0 , qsol=  79.1 , voltage_to_load=  193.7 , power=  474.8 ,  predicted voltage_to_load =  159.23504285812376 , predicted power=  299.5003975152969\n",
      "row [0] data:  M=  0.0 , T1=  30.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  44.42865 , power=  281.98642011749996 ,  predicted voltage_to_load =  37.58865088522434 , predicted power=  218.5742288708687\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  200.0 , qsol=  96.2 , voltage_to_load=  88.32577500000001 , power=  81.096076188675 ,  predicted voltage_to_load =  63.123067986965175 , predicted power=  39.52405386120081\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  700.0 , qsol=  14.3 , voltage_to_load=  90.42537 , power=  571.8005272473356 ,  predicted voltage_to_load =  65.4094904243946 , predicted power=  465.7113979816437\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  97.431525 , power=  376.7024628502231 ,  predicted voltage_to_load =  65.43618903160095 , predicted power=  213.27697045803072\n",
      "row [0] data:  M=  2.0 , T1=  20.0 , gam=  700.0 , qsol=  14.3 , voltage_to_load=  59.774 , power=  249.85531999999998 ,  predicted voltage_to_load =  38.246790289878845 , predicted power=  219.3731141805649\n",
      "row [0] data:  M=  0.0 , T1=  20.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  48.7157625 , power=  57.0486902874136 ,  predicted voltage_to_load =  37.05797579884529 , predicted power=  18.616710695624352\n",
      "row [0] data:  M=  2.0 , T1=  30.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  124.14599999999999 , power=  518.9302799 ,  predicted voltage_to_load =  126.57640714645386 , predicted power=  556.6063731908798\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  92.1 , power=  49.9 ,  predicted voltage_to_load =  54.10247577428817 , predicted power=  11.036666309833526\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  84.50155 , power=  41.978318356275715 ,  predicted voltage_to_load =  47.42271238267421 , predicted power=  3.870528993010521\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  700.0 , qsol=  55.3 , voltage_to_load=  90.42537 , power=  147.86161916160762 ,  predicted voltage_to_load =  57.605522382259366 , predicted power=  55.628375905752186\n",
      "row [0] data:  M=  1.0 , T1=  20.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  20.482 , power=  171.22951999999998 ,  predicted voltage_to_load =  22.70523420125246 , predicted power=  188.00871337652208\n",
      "row [0] data:  M=  2.0 , T1=  20.0 , gam=  1000.0 , qsol=  11.7 , voltage_to_load=  69.03 , power=  407.27700000000004 ,  predicted voltage_to_load =  62.97783966064453 , predicted power=  366.16968753337864\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  500.0 , qsol=  43.2 , voltage_to_load=  48.4 , power=  54.3 ,  predicted voltage_to_load =  39.63940436840057 , predicted power=  27.420351004600526\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  124.10000000000001 , power=  518.9 ,  predicted voltage_to_load =  124.45535039901733 , predicted power=  561.5914031982422\n",
      "row [0] data:  M=  1.0 , T1=  30.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  93.21305 , power=  208.86232428611777 ,  predicted voltage_to_load =  55.70383338332176 , predicted power=  75.73948739469051\n",
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  500.0 , qsol=  21.2 , voltage_to_load=  96.8 , power=  442.9 ,  predicted voltage_to_load =  73.04795471429824 , predicted power=  290.2667293787003\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  500.0 , qsol=  43.2 , voltage_to_load=  128.7 , power=  383.6 ,  predicted voltage_to_load =  122.00452213287353 , predicted power=  330.2077717065811\n",
      "row [0] data:  M=  0.0 , T1=  10.0 , gam=  700.0 , qsol=  55.3 , voltage_to_load=  49.305 , power=  43.9 ,  predicted voltage_to_load =  37.8322748541832 , predicted power=  15.930602341890335\n",
      "row [0] data:  M=  0.0 , T1=  30.0 , gam=  1000.0 , qsol=  3.92 , voltage_to_load=  46.606525 , power=  554.1245338203124 ,  predicted voltage_to_load =  37.22537297606468 , predicted power=  457.17894554138184\n",
      "row [0] data:  M=  2.0 , T1=  10.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  20.8 , power=  62.1 ,  predicted voltage_to_load =  24.197336915135384 , predicted power=  82.72715731859208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row [0] data:  M=  1.0 , T1=  10.0 , gam=  500.0 , qsol=  79.1 , voltage_to_load=  96.8 , power=  118.7 ,  predicted voltage_to_load =  69.50755898356438 , predicted power=  57.58875167965889\n",
      "row [0] data:  M=  0.0 , T1=  30.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  45.212685 , power=  417.1809969202499 ,  predicted voltage_to_load =  36.14745862185955 , predicted power=  319.01412571668624\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    \n",
    "    test = [[ X_train[i][0] , X_train[i][1] , X_train[i][2],X_train[i][3] ]]\n",
    "    testarray = np.array(test)\n",
    "    outpt = model.predict(testarray)\n",
    "    print ('row [0] data:  M= ', X_train[i][0],', T1= ', X_train[i][1]*Tamed, ', gam= ', X_train[i][2]*IDmed, \\\n",
    "        ', qsol= ', X_train[i][3]*RLmed,', voltage_to_load= ', Y_train[i][0]*VLmed,\\\n",
    "        ', power= ', Y_train[i][1]*Wdmed,',  predicted voltage_to_load = ', outpt[0][0]*VLmed,\\\n",
    "           ', predicted power= ', outpt[0][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=[]\n",
    "prepower=[]\n",
    "test=[]\n",
    "for i in range(len(X_train)):\n",
    "\n",
    "    test = [[ X_train[i][0] , X_train[i][1] , X_train[i][2],X_train[i][3] ]]\n",
    "    testarray = np.array(test)\n",
    "    outpt = model.predict(testarray)\n",
    "    power.append(Y_train[i][1]*Wdmed)\n",
    "    prepower.append(outpt[0][1]*Wdmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(power,prepower,s=10, label='power output comparison')\n",
    "plt.ylabel(r'data value power output,w ',fontsize = 16)\n",
    "plt.xlabel(r'predicted power output,w',fontsize = 16)\n",
    "plt.title('predicted power output vs. data value power output',fontsize=16)\n",
    "plt.legend(loc='upper left')\n",
    "plt.loglog()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row [0] data:  M=  0.0 ,T1=  10.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  50.8 , power=  102.5 ,  test voltage_to_load =  44.42865 , test power=  45.69224400052082\n",
      "row [0] data:  M=  2.0 ,T1=  10.0 , gam=  200.0 , qsol=  24.3 , voltage_to_load=  27.899999999999995 , power=  32.1 ,  test voltage_to_load =  46.439325 , test power=  27.264360385026865\n",
      "row [0] data:  M=  0.0 ,T1=  10.0 , gam=  200.0 , qsol=  51.8 , voltage_to_load=  46.0 , power=  40.9 ,  test voltage_to_load =  63.175999999999995 , test power=  188.26448\n",
      "row [0] data:  M=  2.0 ,T1=  20.0 , gam=  500.0 , qsol=  79.1 , voltage_to_load=  185.7573 , power=  436.22976616042985 ,  test voltage_to_load =  46.606525 , test power=  86.19714970538195\n",
      "row [0] data:  M=  1.0 ,T1=  10.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  101.6 , power=  248.3 ,  test voltage_to_load =  59.5 , test power=  68.5\n",
      "row [0] data:  M=  2.0 ,T1=  20.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  194.86305 , power=  912.7790445986176 ,  test voltage_to_load =  189.03537 , test power=  646.1911593315895\n",
      "row [0] data:  M=  1.0 ,T1=  10.0 , gam=  200.0 , qsol=  51.8 , voltage_to_load=  92.1 , power=  163.9 ,  test voltage_to_load =  92.1 , test power=  88.2\n",
      "row [0] data:  M=  2.0 ,T1=  10.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  184.2 , power=  199.6 ,  test voltage_to_load =  50.8 , test power=  62.0\n",
      "row [0] data:  M=  0.0 ,T1=  20.0 , gam=  500.0 , qsol=  21.2 , voltage_to_load=  46.439325 , power=  101.72692954979364 ,  test voltage_to_load =  44.162887500000004 , test power=  37.65174965902\n",
      "row [0] data:  M=  2.0 ,T1=  10.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  148.6 , power=  877.2 ,  test voltage_to_load =  98.6 , test power=  175.8\n",
      "row [0] data:  M=  0.0 ,T1=  20.0 , gam=  200.0 , qsol=  24.3 , voltage_to_load=  44.162887500000004 , power=  80.26175441718752 ,  test voltage_to_load =  63.099999999999994 , test power=  188.19999999999996\n",
      "row [0] data:  M=  2.0 ,T1=  10.0 , gam=  1000.0 , qsol=  3.92 , voltage_to_load=  23.1 , power=  136.4 ,  test voltage_to_load =  46.0 , test power=  22.0\n",
      "row [0] data:  M=  0.0 ,T1=  20.0 , gam=  200.0 , qsol=  170.1 , voltage_to_load=  44.162887500000004 , power=  11.465964916775999 ,  test voltage_to_load =  27.945 , test power=  32.13675\n",
      "row [0] data:  M=  1.0 ,T1=  10.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  98.6 , power=  327.4 ,  test voltage_to_load =  50.8 , test power=  220.70000000000002\n",
      "row [0] data:  M=  0.0 ,T1=  10.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  49.3 , power=  81.8 ,  test voltage_to_load =  48.4 , test power=  335.3\n",
      "row [0] data:  M=  2.0 ,T1=  30.0 , gam=  200.0 , qsol=  96.2 , voltage_to_load=  110.63 , power=  127.22449999999999 ,  test voltage_to_load =  59.70000000000001 , test power=  249.79999999999998\n",
      "row [0] data:  M=  0.0 ,T1=  30.0 , gam=  200.0 , qsol=  96.2 , voltage_to_load=  42.250775 , power=  18.556423992729986 ,  test voltage_to_load =  47.2588425 , test power=  40.38694745822434\n",
      "row [0] data:  M=  2.0 ,T1=  10.0 , gam=  1000.0 , qsol=  41.6 , voltage_to_load=  203.2 , power=  993.5 ,  test voltage_to_load =  88.8573 , test power=  372.4348944948113\n",
      "row [0] data:  M=  2.0 ,T1=  10.0 , gam=  700.0 , qsol=  4.9 , voltage_to_load=  20.4 , power=  85.6 ,  test voltage_to_load =  23.1 , test power=  272.9\n",
      "row [0] data:  M=  2.0 ,T1=  30.0 , gam=  1000.0 , qsol=  25.2 , voltage_to_load=  148.68 , power=  877.2120000000001 ,  test voltage_to_load =  101.6 , test power=  883.1\n",
      "row [0] data:  M=  0.0 ,T1=  30.0 , gam=  700.0 , qsol=  29.700000000000003 , voltage_to_load=  45.212685 , power=  68.82784124273485 ,  test voltage_to_load =  98.6 , test power=  679.9\n",
      "row [0] data:  M=  2.0 ,T1=  30.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  20.86 , power=  62.16279999999999 ,  test voltage_to_load =  48.7157625 , test power=  202.8397876885817\n",
      "row [0] data:  M=  1.0 ,T1=  30.0 , gam=  1000.0 , qsol=  11.7 , voltage_to_load=  93.21305 , power=  742.6215974617521 ,  test voltage_to_load =  46.0 , test power=  87.3\n",
      "row [0] data:  M=  1.0 ,T1=  20.0 , gam=  500.0 , qsol=  7.000000000000001 , voltage_to_load=  20.86 , power=  124.32559999999998 ,  test voltage_to_load =  49.3 , test power=  169.9\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    test = [[ X_train[i][0] , X_train[i][1] , X_train[i][2],X_train[i][3] ]]\n",
    "    testarray = np.array(test)\n",
    "    print ('row [0] data:  M= ', X_train[i][0],',T1= ', X_train[i][1]*Tamed, ', gam= ', X_train[i][2]*IDmed, \\\n",
    "        ', qsol= ', X_train[i][3]*RLmed ,', voltage_to_load= ', Y_train[i][0]*VLmed,\\\n",
    "        ', power= ', Y_train[i][1]*Wdmed,',  test voltage_to_load = ', Y_test[i][0]*VLmed,\\\n",
    "           ', test power= ', Y_test[i][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "power2=[]\n",
    "testpower=[]\n",
    "for i in range(len(Y_test)):\n",
    "    test = [[ X_train[i][0] , X_train[i][1] , X_train[i][2],X_train[i][3] ]]\n",
    "    testarray = np.array(test)\n",
    "    power2.append( Y_train[i][1]*Wdmed)\n",
    "    testpower.append(Y_test[i][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(testpower,power2,s=10, label='power output comparison')\n",
    "plt.ylabel(r'normalized value power output,w ',fontsize = 16)\n",
    "plt.xlabel(r'predicted power output,w',fontsize = 16)\n",
    "plt.title('predicted power output vs. normalized value power output',fontsize=16)\n",
    "plt.legend(loc='upper left')\n",
    "plt.loglog()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicted power=  29.853730031847956\n",
      " predicted power=  -2.93679336309433\n",
      " predicted power=  43.94901807308197\n",
      " predicted power=  6.691477718949318\n",
      " predicted power=  35.80441387593746\n",
      " predicted power=  13.765411856770516\n",
      " predicted power=  162.99013158082963\n",
      " predicted power=  43.02474797964096\n",
      " predicted power=  20.890856817364693\n"
     ]
    }
   ],
   "source": [
    "test1 = []\n",
    "outpt1=[]\n",
    "X_train1 = [[0,10,200,50],[0,20,200,130],[0,10,50,40],[0,20,500,80],[0,20,700,30],[0,20,700,55],[0,10,1000,12],[0,20,1000,25],[0,20,1000,39]]\n",
    "for i in range(len(X_train1)):\n",
    "    test1 = [[ 0, X_train1[i][1]/Tamed , X_train1[i][2]/IDmed,X_train1[i][3]/RLmed ]]\n",
    "    testarray1 = np.array(test1)\n",
    "    outpt1 = model.predict(testarray1)\n",
    "    print ( ' predicted power= ', outpt1[0][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicted power=  29.853730031847956\n",
      " predicted power=  -2.93679336309433\n",
      " predicted power=  43.94901807308197\n",
      " predicted power=  6.691477718949318\n",
      " predicted power=  35.80441387593746\n",
      " predicted power=  13.765411856770516\n",
      " predicted power=  162.99013158082963\n",
      " predicted power=  43.02474797964096\n",
      " predicted power=  20.890856817364693\n"
     ]
    }
   ],
   "source": [
    "test2 = []\n",
    "outpt2=[]\n",
    "X_train2 = [[1,10,200,50],[1,20,200,130],[1,10,50,40],[1,20,500,80],[1,20,700,30],[1,20,700,55],[1,10,1000,12],[1,20,1000,25],[1,20,1000,39]]\n",
    "for i in range(len(X_train2)):\n",
    "    test2 = [[ 0, X_train2[i][1]/Tamed , X_train2[i][2]/IDmed,X_train2[i][3]/RLmed ]]\n",
    "    testarray2 = np.array(test2)\n",
    "    outpt2 = model.predict(testarray2)\n",
    "    print ( ' predicted power= ', outpt2[0][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicted power=  29.853730031847956\n",
      " predicted power=  -2.93679336309433\n",
      " predicted power=  43.94901807308197\n",
      " predicted power=  6.691477718949318\n",
      " predicted power=  35.80441387593746\n",
      " predicted power=  13.765411856770516\n",
      " predicted power=  162.99013158082963\n",
      " predicted power=  43.02474797964096\n",
      " predicted power=  20.890856817364693\n"
     ]
    }
   ],
   "source": [
    "test3 = []\n",
    "outpt3=[]\n",
    "X_train3 = [[2,10,200,50],[2,20,200,130],[2,10,50,40],[2,20,500,80],[2,20,700,30],[2,20,700,55],[2,10,1000,12],[2,20,1000,25],[2,20,1000,39]]\n",
    "for i in range(len(X_train3)):\n",
    "    test3 = [[ 0, X_train3[i][1]/Tamed , X_train3[i][2]/IDmed,X_train3[i][3]/RLmed ]]\n",
    "    testarray3 = np.array(test3)\n",
    "    outpt3 = model.predict(testarray3)\n",
    "    print ( ' predicted power= ', outpt3[0][1]*Wdmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
